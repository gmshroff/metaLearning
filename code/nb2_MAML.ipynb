{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gmshroff/metaLearning2022/blob/main/code/nb2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa5GVdJI1dhL"
   },
   "source": [
    "# MAML - MODEL-AGNOSTIC META-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UhuOxjay1dhO"
   },
   "outputs": [],
   "source": [
    "# !pip install import_ipynb --quiet\n",
    "# !pip install learn2learn --quiet!pip install import_ipynb --quiet\n",
    "# !git clone https://github.com/gmshroff/metaLearning.git\n",
    "# %cd metaLearning/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-d_vlK7d13vJ"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWJRPCsO1dhO"
   },
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "import utils\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "75kCLSpo1dhP"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from l2lutils import KShotLoader\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seBbH_9y4SQH"
   },
   "outputs": [],
   "source": [
    "from course_data import MyDS, TsDS, FeedData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qz_c-8Sq1dhP"
   },
   "source": [
    "# Pre-trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtFHhHPF1dhQ"
   },
   "outputs": [],
   "source": [
    "#Generate data - euclidean\n",
    "meta_train_ds, meta_test_ds, full_loader = utils.euclideanDataset(n_samples=10000,n_features=20,n_classes=10,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XrrwgPQd1dhQ"
   },
   "outputs": [],
   "source": [
    "# Define an MLP network. Note that input dimension has to be data dimension. For classification\n",
    "# final dimension has to be number of classes; for regression one.\n",
    "#torch.manual_seed(10)\n",
    "net0 = models.MLP(dims=[20,32,32,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKo0Fyev1dhR"
   },
   "outputs": [],
   "source": [
    "# Train the network; note that network is trained in place so repeated calls further train it.\n",
    "net0,loss,accs=models.Train(net0,full_loader,lr=1e-3,epochs=10,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNieUD4s1dhR"
   },
   "outputs": [],
   "source": [
    "#Training accuracy.\n",
    "models.accuracy(net0,meta_train_ds.samples,meta_train_ds.labels,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V507W4Qn1dhR"
   },
   "outputs": [],
   "source": [
    "# Test accuracy.\n",
    "models.accuracy(net0,meta_test_ds.samples,meta_test_ds.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkPojWSl1dhR"
   },
   "source": [
    "# Second-order Differentiation using Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jN4qmFhZ1dhS"
   },
   "source": [
    "Second-order derivatives as needed for MAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwaukhc61dhS"
   },
   "outputs": [],
   "source": [
    "network = (lambda x,w: x@w)\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HG2V_Kdn1dhS"
   },
   "outputs": [],
   "source": [
    "Z=(torch.ones(3,1)).float()\n",
    "z=(torch.ones(3,1)*2).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCW_-ble1dhS"
   },
   "outputs": [],
   "source": [
    "Zt=(torch.ones(3,1)*1.5).float()\n",
    "zt=(torch.ones(3,1)*2*1.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0y8ixP91dhS"
   },
   "outputs": [],
   "source": [
    "w0=(torch.ones(1,1,requires_grad=True)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNEYoDBX1dhS"
   },
   "outputs": [],
   "source": [
    "w1=w0.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L08I5G181dhS"
   },
   "outputs": [],
   "source": [
    "L=loss(network(Z,w1),z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iOjA3gB1dhT"
   },
   "outputs": [],
   "source": [
    "#g=torch.autograd.grad(L,w0)[0]\n",
    "g=torch.autograd.grad(L,w1,create_graph=True)[0]\n",
    "# L.backward()# Not good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ForDE6Nd1dhU"
   },
   "outputs": [],
   "source": [
    "w1.grad, w0.grad, L, w0, w1,w1.requires_grad,g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6osHThu71dhU"
   },
   "outputs": [],
   "source": [
    "w1 = w1 - 0.1*g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWRJUeR71dhU"
   },
   "outputs": [],
   "source": [
    "L1=loss(network(Zt,w1),zt)\n",
    "#L1=loss(net(Zt,w0-0.1*(2.0*(w0-2.0))),zt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2mx-mdt1dhU"
   },
   "outputs": [],
   "source": [
    "# Both OK - latter used with optimizer.step()\n",
    "g1=torch.autograd.grad(L1,w0)[0]\n",
    "# L1.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnuSZcD61dhU"
   },
   "outputs": [],
   "source": [
    "g1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VglIzSwP1dhU"
   },
   "source": [
    "Working this out manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXkTpRGP1dhU"
   },
   "source": [
    "$w_0=1, L=(w_0-2)^2, dL=2\\times(w_0-2)=-2,w_1=w_0-0.1\\times(-2)=1.2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olkhaz0P1dhU"
   },
   "source": [
    "$L_1=(w_1\\times1.5-3)^2 = (w_0-0.1\\times(2\\times(w_0-2))\\times1.5-3)^2 = (-1.2)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIGfBmaE1dhU"
   },
   "source": [
    "$dL_1 = 2 \\times (-1.2) \\times (1.5 \\times (1-.2))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "UU4fhxj51dhU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.8800000000000003"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*(-1.2)*(1.5*(1-.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "Jb0NZIjF1dhU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.8800]]), None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0.grad,w1.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4O4etDg1dhU"
   },
   "source": [
    "# Meta-Learning: Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJ3cPWsX1dhV"
   },
   "source": [
    "Generate a k-shot n-way loader using the meta-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "l0xoyHhO1dhV"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4], [5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_train = [i for i in range(5)]\n",
    "classes_test = [i+5 for i in range(5)]\n",
    "classes_train, classes_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "N23RO_0K1dhV"
   },
   "outputs": [],
   "source": [
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=5,ways=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHaNmJNJ1dhV"
   },
   "source": [
    "Sample a task - each task has a k-shot n-way training set and a similar test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "Y3-vYPEs1dhV"
   },
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxmfnsiY1dhV"
   },
   "source": [
    "Let's try directly learning using the task training set albeit its small size: create a dataset and loader and train it with the earlier network and Train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "xxGgmtqy1dhV"
   },
   "outputs": [],
   "source": [
    "taskds = utils.MyDS(d_train[0],d_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "XbYtBB-L1dhV"
   },
   "outputs": [],
   "source": [
    "d_train_loader = torch.utils.data.DataLoader(dataset=taskds,batch_size=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "QzKDWt5B1dhV"
   },
   "outputs": [],
   "source": [
    "net = models.MLP(dims=[20,32,32,10])\n",
    "# net1 = models.MLP(dims=[400,32,32,len(mapping.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "Vtlkqey71dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   19 Loss: 3.36018e-02 Accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "net,losses,accs=models.Train(net,d_train_loader,lr=1e-3,epochs=20,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "d0al3FLJ1dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(net,d_train[0],d_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3vt8hyp1dhV"
   },
   "source": [
    "How does it do on the test set of the sampled task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "id": "R5irGQny1dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.72"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(net,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it do on the test set of from the meta-test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "5Zwm4g551dhV"
   },
   "outputs": [],
   "source": [
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=5,ways=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "kcza3qHx1dhV"
   },
   "outputs": [],
   "source": [
    "d_train,d_test=meta_test_kloader.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "DYWNXNH71dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.24"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(net,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with a pre-trained network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "netp=net0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "Vtlkqey71dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   19 Loss: 7.62113e-03 Accuracy: 1.00000\n"
     ]
    }
   ],
   "source": [
    "netp,losses,accs=models.Train(netp,d_train_loader,lr=1e-3,epochs=20,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "d0al3FLJ1dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(netp,d_train[0],d_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3vt8hyp1dhV"
   },
   "source": [
    "How does it do on the test set of the sampled task?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "R5irGQny1dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(netp,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "id": "5Zwm4g551dhV"
   },
   "outputs": [],
   "source": [
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=5,ways=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "id": "kcza3qHx1dhV"
   },
   "outputs": [],
   "source": [
    "d_train,d_test=meta_test_kloader.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "id": "DYWNXNH71dhV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models.accuracy(netp,d_test[0],d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLZaHZof1dhV"
   },
   "source": [
    "# MAML - Model-Agnostic Meta-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "id": "dfrGrb4N1dhW"
   },
   "outputs": [],
   "source": [
    "import learn2learn as l2l\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "id": "9dvjsq621dhW"
   },
   "outputs": [],
   "source": [
    "shots,ways=2,5\n",
    "net = models.MLP(dims=[20,32,32,ways])\n",
    "maml = l2l.algorithms.MAML(net, lr=1e-1)\n",
    "optimizer = optim.Adam(net.parameters(),lr=1e-3)\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=shots,ways=ways)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vLP9R711dhW"
   },
   "source": [
    "The MAML class above wraps our nn.Module class for parameter cloning and other purposes as below. One iteration of the MAML algorithm proceeds by first sampling a training task: Note that each of d_train and d_test is a tuple comprising of a training set, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "id": "-jUYDjQP1dhW"
   },
   "outputs": [],
   "source": [
    "d_train,d_test=meta_train_kloader.get_task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "id": "9nkCEqji1dhW"
   },
   "outputs": [],
   "source": [
    "learner = maml.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aB0AAZB1dhW"
   },
   "source": [
    "The learner class above is a 'clone' of our network with copies of parameters so that we can change these without changing the parameters of the network. We apply the learner on training data of d_train and compute TRAINING loss w.r.t the training data of the task, i.e., d_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "kpuWqRR31dhW"
   },
   "outputs": [],
   "source": [
    "train_preds = learner(d_train[0])\n",
    "train_loss = lossfn(train_preds,d_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "id": "fWr-HQqc6Oa7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.7114, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "pwgwmXlq1dhW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1121, -0.0092, -0.1242,  0.0586,  0.0297, -0.0524, -0.2009,  0.2082,\n",
       "          0.0989,  0.0167, -0.0350,  0.0734, -0.0245,  0.1639, -0.0037,  0.1735,\n",
       "          0.0533, -0.0820,  0.1081,  0.0737],\n",
       "        [ 0.0070, -0.0386, -0.0314, -0.0761, -0.0223, -0.0870,  0.1188, -0.0043,\n",
       "          0.1373,  0.0178,  0.1070,  0.1092, -0.0285, -0.1202,  0.1345,  0.1904,\n",
       "          0.0238,  0.2207, -0.0216,  0.1676],\n",
       "        [ 0.0640,  0.0818,  0.0549, -0.0860, -0.1960,  0.2120,  0.0915, -0.2207,\n",
       "          0.1724,  0.2092,  0.1118, -0.0383, -0.1833,  0.0796, -0.1369, -0.0677,\n",
       "          0.0626, -0.0314, -0.2176, -0.0437],\n",
       "        [ 0.0673, -0.0091, -0.0399,  0.0687, -0.0365,  0.1350,  0.0106,  0.1543,\n",
       "          0.0619, -0.1191, -0.1569, -0.0604,  0.1461,  0.1343,  0.1576, -0.1636,\n",
       "          0.0497, -0.1160,  0.0541, -0.0714],\n",
       "        [-0.0323,  0.1176, -0.0047, -0.2212,  0.1174,  0.0010,  0.2144,  0.0933,\n",
       "         -0.0245,  0.1226, -0.1737, -0.1152,  0.0480, -0.0632,  0.1881,  0.0511,\n",
       "         -0.1703, -0.1568,  0.0504,  0.0414],\n",
       "        [ 0.1063,  0.0285,  0.1204,  0.1526,  0.1802, -0.0356,  0.2101, -0.0953,\n",
       "         -0.0884, -0.1013,  0.0938, -0.0885,  0.0592,  0.0450, -0.0909,  0.1788,\n",
       "         -0.1746,  0.1772, -0.0168,  0.0737],\n",
       "        [-0.0592,  0.2223,  0.1121, -0.2142, -0.0324,  0.0700, -0.1042, -0.1266,\n",
       "          0.1930,  0.0924, -0.1438,  0.1512,  0.0449, -0.1716,  0.0058, -0.0548,\n",
       "          0.2211, -0.0635,  0.1538,  0.0957],\n",
       "        [ 0.0972, -0.0804,  0.0301,  0.1933, -0.0204, -0.1160, -0.0979,  0.1716,\n",
       "         -0.2188, -0.1354,  0.1900,  0.0960,  0.1273, -0.1636, -0.1743, -0.2121,\n",
       "         -0.1217,  0.0676,  0.1755, -0.2158],\n",
       "        [ 0.0651,  0.1235, -0.0408, -0.1848,  0.1666, -0.0847,  0.1439, -0.0381,\n",
       "          0.0637,  0.0457,  0.0026,  0.1533,  0.1718, -0.1449,  0.2135,  0.0516,\n",
       "          0.0014,  0.1185, -0.0504, -0.1407],\n",
       "        [ 0.0772,  0.0727, -0.1929, -0.0257, -0.0850,  0.0247, -0.1555, -0.2088,\n",
       "         -0.1368,  0.0200, -0.1994, -0.0129, -0.1466,  0.2112, -0.0307, -0.1194,\n",
       "          0.1225, -0.0967, -0.0563, -0.0568],\n",
       "        [ 0.0977,  0.0827,  0.1660,  0.1687,  0.1922,  0.0298,  0.1275, -0.1756,\n",
       "         -0.0317, -0.0921,  0.1230, -0.1583, -0.1927, -0.1669, -0.0611, -0.0327,\n",
       "         -0.0338, -0.2225, -0.0970,  0.1200],\n",
       "        [-0.0539, -0.1353, -0.0497,  0.0277,  0.0377,  0.1261,  0.0028, -0.0833,\n",
       "         -0.0626, -0.0685, -0.0467,  0.0568,  0.1332,  0.1412, -0.0718, -0.1758,\n",
       "          0.0834, -0.1357, -0.1259,  0.0016],\n",
       "        [-0.1623,  0.0845, -0.1108,  0.1130, -0.1630, -0.1880,  0.0575, -0.1740,\n",
       "         -0.0760, -0.0927, -0.2050,  0.2170,  0.1184, -0.0250, -0.1385, -0.0311,\n",
       "          0.1072,  0.1406, -0.1894,  0.1746],\n",
       "        [ 0.0042,  0.1373, -0.0241,  0.1655,  0.0081, -0.0499, -0.1301,  0.0956,\n",
       "         -0.0088,  0.1484,  0.0023, -0.0085, -0.1437,  0.0987, -0.0684, -0.1928,\n",
       "         -0.0715,  0.2107, -0.0481, -0.2140],\n",
       "        [ 0.0771,  0.0217,  0.1903,  0.1569, -0.1790,  0.0782, -0.1510,  0.1426,\n",
       "          0.1502,  0.0515, -0.0519, -0.2219,  0.0113,  0.1779,  0.1683,  0.0644,\n",
       "         -0.0529,  0.1253,  0.1926, -0.1375],\n",
       "        [ 0.0009,  0.0751,  0.1472,  0.1744, -0.0034, -0.0018,  0.0036,  0.0754,\n",
       "          0.1114,  0.1544, -0.0190,  0.2025, -0.0069, -0.1068,  0.1411, -0.0003,\n",
       "          0.1121, -0.0311,  0.0036, -0.0532],\n",
       "        [ 0.1098, -0.1391,  0.0688, -0.1226,  0.0533, -0.0904,  0.0155,  0.0973,\n",
       "          0.0648, -0.0846, -0.0755,  0.0101, -0.0468, -0.0210,  0.1550,  0.0626,\n",
       "          0.2093,  0.0609, -0.0095, -0.0225],\n",
       "        [-0.0680, -0.2206,  0.1427,  0.0402,  0.1437,  0.1117, -0.0978, -0.1872,\n",
       "         -0.0932,  0.0581, -0.1687, -0.0860,  0.1296, -0.1298,  0.0127,  0.0786,\n",
       "          0.0828,  0.1933, -0.2003,  0.1153],\n",
       "        [-0.2047,  0.0644,  0.0990, -0.0473, -0.1955,  0.0155, -0.1819, -0.0990,\n",
       "          0.2172, -0.0125, -0.1625,  0.1375, -0.0006,  0.1782,  0.1945,  0.0731,\n",
       "         -0.2228, -0.1001,  0.1592, -0.0965],\n",
       "        [ 0.1608, -0.2164, -0.1158,  0.0293,  0.2094,  0.0447,  0.2099,  0.0621,\n",
       "         -0.1843,  0.1263, -0.0768,  0.0422,  0.2017,  0.1006,  0.1443, -0.0546,\n",
       "          0.2172, -0.0854,  0.1499,  0.0157],\n",
       "        [ 0.1066,  0.0464, -0.0703,  0.1504,  0.0898, -0.2159, -0.0937, -0.2140,\n",
       "         -0.0855, -0.0602, -0.1987,  0.0042, -0.2038, -0.1731,  0.0800,  0.1264,\n",
       "          0.0266,  0.0079,  0.0871, -0.0463],\n",
       "        [ 0.0339, -0.0149,  0.0257,  0.1161,  0.1920, -0.0583, -0.1670, -0.0199,\n",
       "         -0.1166,  0.2085,  0.0682,  0.1068, -0.2068,  0.1667, -0.2074, -0.0393,\n",
       "         -0.1407,  0.0613,  0.1326, -0.2214],\n",
       "        [ 0.0817,  0.1494, -0.2218, -0.0654, -0.1138, -0.1346, -0.1555,  0.1564,\n",
       "          0.0718, -0.0034, -0.0860, -0.1221,  0.1924, -0.0319,  0.2008,  0.1847,\n",
       "          0.1722,  0.1880,  0.0399,  0.1082],\n",
       "        [-0.1535, -0.0918,  0.1032, -0.0986,  0.0883, -0.1052,  0.0704, -0.0631,\n",
       "         -0.0738, -0.1097,  0.1640,  0.1455, -0.1284,  0.2118, -0.2225,  0.1830,\n",
       "          0.1541, -0.0737, -0.0954, -0.2002],\n",
       "        [ 0.1770, -0.1780, -0.0802, -0.2109, -0.1843,  0.1652, -0.0148, -0.0268,\n",
       "          0.0658,  0.1521,  0.1570,  0.0538,  0.1726,  0.1680, -0.1211, -0.0309,\n",
       "          0.2215, -0.1553,  0.2178, -0.0256],\n",
       "        [-0.0621,  0.0606, -0.1004,  0.0736,  0.0125, -0.1564, -0.2161,  0.0278,\n",
       "         -0.1811,  0.2128,  0.1911, -0.1997,  0.1761,  0.0845,  0.0994, -0.1379,\n",
       "          0.2146,  0.1780, -0.0965, -0.0842],\n",
       "        [-0.0078,  0.2082, -0.0260, -0.0872,  0.1913,  0.0197,  0.1830,  0.0537,\n",
       "          0.1995, -0.0201,  0.1400, -0.0192, -0.0859, -0.1375,  0.2182, -0.0220,\n",
       "          0.1905,  0.1733, -0.0338, -0.2059],\n",
       "        [-0.0753,  0.1981,  0.2103,  0.1225, -0.1874, -0.1702,  0.0733, -0.0333,\n",
       "         -0.1135,  0.1629, -0.0257,  0.0373, -0.1762, -0.1050,  0.0992,  0.1183,\n",
       "          0.1532, -0.2160,  0.0441, -0.1516],\n",
       "        [-0.0950,  0.1203,  0.2140, -0.1714,  0.1462,  0.2088,  0.1904,  0.2001,\n",
       "         -0.1519, -0.1127,  0.2101, -0.1185, -0.2197,  0.0271, -0.0304, -0.1276,\n",
       "         -0.0640,  0.2098,  0.1996, -0.0675],\n",
       "        [ 0.1295, -0.0311,  0.1008,  0.1051, -0.0440,  0.2042, -0.1974, -0.0962,\n",
       "          0.0765, -0.1132,  0.1317,  0.0486,  0.1649,  0.2143,  0.0385, -0.1161,\n",
       "          0.0249,  0.1846, -0.1740, -0.2168],\n",
       "        [-0.1060, -0.0709, -0.0586, -0.0186, -0.1112,  0.0322, -0.1299, -0.1227,\n",
       "          0.0626,  0.1174, -0.0065, -0.0740,  0.2067, -0.0169, -0.1485, -0.0037,\n",
       "         -0.0547, -0.1546, -0.1635, -0.1644],\n",
       "        [ 0.1539, -0.0073,  0.1099, -0.0464,  0.1213,  0.1192,  0.1787,  0.0897,\n",
       "          0.0491, -0.0895, -0.1761,  0.0244, -0.1741, -0.2123, -0.1298, -0.0409,\n",
       "         -0.1830,  0.0866,  0.0423,  0.0561]], requires_grad=True)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "I__tZJcB1dhW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1121, -0.0092, -0.1242,  0.0586,  0.0297, -0.0524, -0.2009,  0.2082,\n",
       "          0.0989,  0.0167, -0.0350,  0.0734, -0.0245,  0.1639, -0.0037,  0.1735,\n",
       "          0.0533, -0.0820,  0.1081,  0.0737],\n",
       "        [ 0.0070, -0.0386, -0.0314, -0.0761, -0.0223, -0.0870,  0.1188, -0.0043,\n",
       "          0.1373,  0.0178,  0.1070,  0.1092, -0.0285, -0.1202,  0.1345,  0.1904,\n",
       "          0.0238,  0.2207, -0.0216,  0.1676],\n",
       "        [ 0.0640,  0.0818,  0.0549, -0.0860, -0.1960,  0.2120,  0.0915, -0.2207,\n",
       "          0.1724,  0.2092,  0.1118, -0.0383, -0.1833,  0.0796, -0.1369, -0.0677,\n",
       "          0.0626, -0.0314, -0.2176, -0.0437],\n",
       "        [ 0.0673, -0.0091, -0.0399,  0.0687, -0.0365,  0.1350,  0.0106,  0.1543,\n",
       "          0.0619, -0.1191, -0.1569, -0.0604,  0.1461,  0.1343,  0.1576, -0.1636,\n",
       "          0.0497, -0.1160,  0.0541, -0.0714],\n",
       "        [-0.0323,  0.1176, -0.0047, -0.2212,  0.1174,  0.0010,  0.2144,  0.0933,\n",
       "         -0.0245,  0.1226, -0.1737, -0.1152,  0.0480, -0.0632,  0.1881,  0.0511,\n",
       "         -0.1703, -0.1568,  0.0504,  0.0414],\n",
       "        [ 0.1063,  0.0285,  0.1204,  0.1526,  0.1802, -0.0356,  0.2101, -0.0953,\n",
       "         -0.0884, -0.1013,  0.0938, -0.0885,  0.0592,  0.0450, -0.0909,  0.1788,\n",
       "         -0.1746,  0.1772, -0.0168,  0.0737],\n",
       "        [-0.0592,  0.2223,  0.1121, -0.2142, -0.0324,  0.0700, -0.1042, -0.1266,\n",
       "          0.1930,  0.0924, -0.1438,  0.1512,  0.0449, -0.1716,  0.0058, -0.0548,\n",
       "          0.2211, -0.0635,  0.1538,  0.0957],\n",
       "        [ 0.0972, -0.0804,  0.0301,  0.1933, -0.0204, -0.1160, -0.0979,  0.1716,\n",
       "         -0.2188, -0.1354,  0.1900,  0.0960,  0.1273, -0.1636, -0.1743, -0.2121,\n",
       "         -0.1217,  0.0676,  0.1755, -0.2158],\n",
       "        [ 0.0651,  0.1235, -0.0408, -0.1848,  0.1666, -0.0847,  0.1439, -0.0381,\n",
       "          0.0637,  0.0457,  0.0026,  0.1533,  0.1718, -0.1449,  0.2135,  0.0516,\n",
       "          0.0014,  0.1185, -0.0504, -0.1407],\n",
       "        [ 0.0772,  0.0727, -0.1929, -0.0257, -0.0850,  0.0247, -0.1555, -0.2088,\n",
       "         -0.1368,  0.0200, -0.1994, -0.0129, -0.1466,  0.2112, -0.0307, -0.1194,\n",
       "          0.1225, -0.0967, -0.0563, -0.0568],\n",
       "        [ 0.0977,  0.0827,  0.1660,  0.1687,  0.1922,  0.0298,  0.1275, -0.1756,\n",
       "         -0.0317, -0.0921,  0.1230, -0.1583, -0.1927, -0.1669, -0.0611, -0.0327,\n",
       "         -0.0338, -0.2225, -0.0970,  0.1200],\n",
       "        [-0.0539, -0.1353, -0.0497,  0.0277,  0.0377,  0.1261,  0.0028, -0.0833,\n",
       "         -0.0626, -0.0685, -0.0467,  0.0568,  0.1332,  0.1412, -0.0718, -0.1758,\n",
       "          0.0834, -0.1357, -0.1259,  0.0016],\n",
       "        [-0.1623,  0.0845, -0.1108,  0.1130, -0.1630, -0.1880,  0.0575, -0.1740,\n",
       "         -0.0760, -0.0927, -0.2050,  0.2170,  0.1184, -0.0250, -0.1385, -0.0311,\n",
       "          0.1072,  0.1406, -0.1894,  0.1746],\n",
       "        [ 0.0042,  0.1373, -0.0241,  0.1655,  0.0081, -0.0499, -0.1301,  0.0956,\n",
       "         -0.0088,  0.1484,  0.0023, -0.0085, -0.1437,  0.0987, -0.0684, -0.1928,\n",
       "         -0.0715,  0.2107, -0.0481, -0.2140],\n",
       "        [ 0.0771,  0.0217,  0.1903,  0.1569, -0.1790,  0.0782, -0.1510,  0.1426,\n",
       "          0.1502,  0.0515, -0.0519, -0.2219,  0.0113,  0.1779,  0.1683,  0.0644,\n",
       "         -0.0529,  0.1253,  0.1926, -0.1375],\n",
       "        [ 0.0009,  0.0751,  0.1472,  0.1744, -0.0034, -0.0018,  0.0036,  0.0754,\n",
       "          0.1114,  0.1544, -0.0190,  0.2025, -0.0069, -0.1068,  0.1411, -0.0003,\n",
       "          0.1121, -0.0311,  0.0036, -0.0532],\n",
       "        [ 0.1098, -0.1391,  0.0688, -0.1226,  0.0533, -0.0904,  0.0155,  0.0973,\n",
       "          0.0648, -0.0846, -0.0755,  0.0101, -0.0468, -0.0210,  0.1550,  0.0626,\n",
       "          0.2093,  0.0609, -0.0095, -0.0225],\n",
       "        [-0.0680, -0.2206,  0.1427,  0.0402,  0.1437,  0.1117, -0.0978, -0.1872,\n",
       "         -0.0932,  0.0581, -0.1687, -0.0860,  0.1296, -0.1298,  0.0127,  0.0786,\n",
       "          0.0828,  0.1933, -0.2003,  0.1153],\n",
       "        [-0.2047,  0.0644,  0.0990, -0.0473, -0.1955,  0.0155, -0.1819, -0.0990,\n",
       "          0.2172, -0.0125, -0.1625,  0.1375, -0.0006,  0.1782,  0.1945,  0.0731,\n",
       "         -0.2228, -0.1001,  0.1592, -0.0965],\n",
       "        [ 0.1608, -0.2164, -0.1158,  0.0293,  0.2094,  0.0447,  0.2099,  0.0621,\n",
       "         -0.1843,  0.1263, -0.0768,  0.0422,  0.2017,  0.1006,  0.1443, -0.0546,\n",
       "          0.2172, -0.0854,  0.1499,  0.0157],\n",
       "        [ 0.1066,  0.0464, -0.0703,  0.1504,  0.0898, -0.2159, -0.0937, -0.2140,\n",
       "         -0.0855, -0.0602, -0.1987,  0.0042, -0.2038, -0.1731,  0.0800,  0.1264,\n",
       "          0.0266,  0.0079,  0.0871, -0.0463],\n",
       "        [ 0.0339, -0.0149,  0.0257,  0.1161,  0.1920, -0.0583, -0.1670, -0.0199,\n",
       "         -0.1166,  0.2085,  0.0682,  0.1068, -0.2068,  0.1667, -0.2074, -0.0393,\n",
       "         -0.1407,  0.0613,  0.1326, -0.2214],\n",
       "        [ 0.0817,  0.1494, -0.2218, -0.0654, -0.1138, -0.1346, -0.1555,  0.1564,\n",
       "          0.0718, -0.0034, -0.0860, -0.1221,  0.1924, -0.0319,  0.2008,  0.1847,\n",
       "          0.1722,  0.1880,  0.0399,  0.1082],\n",
       "        [-0.1535, -0.0918,  0.1032, -0.0986,  0.0883, -0.1052,  0.0704, -0.0631,\n",
       "         -0.0738, -0.1097,  0.1640,  0.1455, -0.1284,  0.2118, -0.2225,  0.1830,\n",
       "          0.1541, -0.0737, -0.0954, -0.2002],\n",
       "        [ 0.1770, -0.1780, -0.0802, -0.2109, -0.1843,  0.1652, -0.0148, -0.0268,\n",
       "          0.0658,  0.1521,  0.1570,  0.0538,  0.1726,  0.1680, -0.1211, -0.0309,\n",
       "          0.2215, -0.1553,  0.2178, -0.0256],\n",
       "        [-0.0621,  0.0606, -0.1004,  0.0736,  0.0125, -0.1564, -0.2161,  0.0278,\n",
       "         -0.1811,  0.2128,  0.1911, -0.1997,  0.1761,  0.0845,  0.0994, -0.1379,\n",
       "          0.2146,  0.1780, -0.0965, -0.0842],\n",
       "        [-0.0078,  0.2082, -0.0260, -0.0872,  0.1913,  0.0197,  0.1830,  0.0537,\n",
       "          0.1995, -0.0201,  0.1400, -0.0192, -0.0859, -0.1375,  0.2182, -0.0220,\n",
       "          0.1905,  0.1733, -0.0338, -0.2059],\n",
       "        [-0.0753,  0.1981,  0.2103,  0.1225, -0.1874, -0.1702,  0.0733, -0.0333,\n",
       "         -0.1135,  0.1629, -0.0257,  0.0373, -0.1762, -0.1050,  0.0992,  0.1183,\n",
       "          0.1532, -0.2160,  0.0441, -0.1516],\n",
       "        [-0.0950,  0.1203,  0.2140, -0.1714,  0.1462,  0.2088,  0.1904,  0.2001,\n",
       "         -0.1519, -0.1127,  0.2101, -0.1185, -0.2197,  0.0271, -0.0304, -0.1276,\n",
       "         -0.0640,  0.2098,  0.1996, -0.0675],\n",
       "        [ 0.1295, -0.0311,  0.1008,  0.1051, -0.0440,  0.2042, -0.1974, -0.0962,\n",
       "          0.0765, -0.1132,  0.1317,  0.0486,  0.1649,  0.2143,  0.0385, -0.1161,\n",
       "          0.0249,  0.1846, -0.1740, -0.2168],\n",
       "        [-0.1060, -0.0709, -0.0586, -0.0186, -0.1112,  0.0322, -0.1299, -0.1227,\n",
       "          0.0626,  0.1174, -0.0065, -0.0740,  0.2067, -0.0169, -0.1485, -0.0037,\n",
       "         -0.0547, -0.1546, -0.1635, -0.1644],\n",
       "        [ 0.1539, -0.0073,  0.1099, -0.0464,  0.1213,  0.1192,  0.1787,  0.0897,\n",
       "          0.0491, -0.0895, -0.1761,  0.0244, -0.1741, -0.2123, -0.1298, -0.0409,\n",
       "         -0.1830,  0.0866,  0.0423,  0.0561]], grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAm8pUTP1dhW"
   },
   "source": [
    "Note that at this point both the learner and original net have the same parameters. Lets see what the gradients w.r.t the TRAINING loss are: (We use pytorch's autograd functions directly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "id": "hiHi7TDV1dhW"
   },
   "outputs": [],
   "source": [
    "from torch.autograd import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "id": "E6MMK_z31dhW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7454e-02, -2.0152e-03,  2.2447e-02,  8.7581e-03, -1.8203e-02,\n",
       "         -5.4251e-03, -2.0568e-02, -1.7881e-02,  2.7123e-02, -1.0518e-02,\n",
       "         -1.1338e-02, -2.3910e-03, -8.3744e-03,  7.0201e-03, -4.0147e-03,\n",
       "          8.0017e-03,  2.0661e-02, -5.4978e-03, -1.6388e-02,  1.7153e-02],\n",
       "        [-1.0287e-02,  1.8980e-02,  3.4639e-03, -4.5314e-03, -3.0747e-02,\n",
       "          4.1525e-03, -1.5390e-02,  1.4088e-02, -3.0416e-02, -1.8706e-03,\n",
       "         -3.7763e-02,  3.3578e-03,  9.0702e-03,  6.6391e-03,  4.5998e-03,\n",
       "          2.8288e-03, -6.1740e-03,  1.1027e-02, -5.6933e-03, -6.0468e-03],\n",
       "        [ 1.3979e-03, -1.3661e-02,  2.8597e-02, -6.5008e-04, -2.2044e-02,\n",
       "         -3.2507e-02, -3.7264e-02, -3.7694e-02,  2.4167e-02, -1.9234e-02,\n",
       "         -8.2191e-02, -2.2443e-02,  2.1345e-02, -6.0700e-03, -2.3837e-02,\n",
       "         -1.0661e-03,  2.5801e-02,  2.7016e-03, -3.1403e-02,  3.0071e-02],\n",
       "        [-1.9711e-02, -1.3235e-02,  2.3637e-02, -2.0479e-02, -2.1933e-03,\n",
       "         -1.1159e-03, -1.2327e-02, -2.0653e-02,  2.1812e-02, -1.8973e-02,\n",
       "         -5.2292e-02,  1.2167e-02, -4.2585e-03, -2.0004e-02, -2.3318e-02,\n",
       "         -1.1022e-02,  1.5485e-02,  4.1399e-03, -1.3377e-02,  5.7085e-05],\n",
       "        [-8.4533e-03, -2.3567e-02,  1.3192e-02,  4.1602e-02,  1.3849e-02,\n",
       "         -2.6015e-02, -1.2728e-03, -4.4832e-02,  1.2878e-02,  2.0017e-02,\n",
       "         -2.0569e-02, -5.0512e-02,  3.6782e-02, -7.6990e-03, -2.4751e-02,\n",
       "          7.7764e-03,  1.7370e-02,  5.8472e-04, -1.9468e-02,  2.4527e-02],\n",
       "        [-2.0444e-02,  6.6934e-03, -7.3265e-03, -2.6096e-03, -1.1996e-02,\n",
       "         -5.9703e-03, -1.0080e-02, -6.4854e-03, -1.8039e-02, -7.3095e-03,\n",
       "         -1.1391e-02,  5.8002e-03,  9.7202e-03,  3.9352e-03, -1.2563e-02,\n",
       "         -1.3352e-03, -3.9527e-03,  1.4911e-02,  3.7273e-03, -7.8379e-03],\n",
       "        [-1.3076e-02, -3.1675e-02,  2.5893e-02,  3.1634e-02, -9.4842e-03,\n",
       "         -6.3243e-03, -3.2301e-02, -2.6687e-02,  3.3901e-02, -1.8767e-02,\n",
       "          4.9724e-03, -6.5511e-03,  2.5602e-02,  1.3602e-02, -2.2917e-02,\n",
       "         -1.0974e-02,  3.7581e-02,  2.9404e-03, -1.5927e-02,  3.9197e-02],\n",
       "        [ 3.3917e-02,  1.8565e-02, -1.2119e-02, -4.0589e-02,  2.4849e-02,\n",
       "          4.2982e-02,  2.1946e-02,  1.2769e-02,  1.9837e-02, -9.6571e-05,\n",
       "          6.7413e-02,  4.4334e-02, -8.0689e-02, -3.6636e-02,  1.8968e-03,\n",
       "          5.4383e-03, -9.9275e-03, -2.6591e-02,  1.9218e-02, -5.2903e-02],\n",
       "        [ 1.0151e-02,  4.2177e-03,  5.4984e-03,  3.1031e-03,  1.5375e-03,\n",
       "         -1.7164e-02,  1.2731e-02, -1.9645e-02, -5.1970e-03, -8.9515e-03,\n",
       "         -2.2995e-02, -1.6491e-02,  1.0492e-02,  2.0342e-04, -1.0110e-02,\n",
       "          1.2062e-02, -8.8868e-03,  6.7879e-03,  7.6554e-03,  7.1750e-03],\n",
       "        [-1.3087e-02, -2.8305e-02,  3.3898e-02,  2.3609e-02, -2.2103e-02,\n",
       "         -3.7672e-02, -4.3765e-02, -6.6500e-02,  5.0724e-02, -8.1886e-03,\n",
       "         -5.4884e-02, -1.4479e-02,  2.3882e-02,  1.1235e-02, -3.6383e-02,\n",
       "         -1.7871e-04,  4.6796e-02,  1.2627e-02, -4.3768e-02,  3.4979e-02],\n",
       "        [ 6.1958e-03,  2.1191e-02, -4.3389e-02, -5.2691e-03,  3.2684e-02,\n",
       "         -5.9000e-03,  5.3648e-02, -3.9831e-03, -1.9411e-02,  2.8984e-02,\n",
       "          6.1986e-02, -1.7005e-02,  8.7410e-03,  1.9792e-03,  3.2225e-04,\n",
       "          2.6940e-02, -4.1294e-02, -1.3378e-02,  2.8630e-02, -1.1307e-02],\n",
       "        [-4.8059e-02,  3.9410e-02,  3.9890e-02,  1.2419e-02, -4.9108e-02,\n",
       "          3.1841e-02,  1.9834e-03, -5.0109e-02, -3.0667e-02,  2.3234e-02,\n",
       "         -6.5422e-02,  3.6257e-03,  1.5889e-02, -1.1335e-02, -5.4607e-02,\n",
       "          4.1986e-02,  1.4098e-04,  2.0299e-02, -2.4348e-02, -2.7917e-02],\n",
       "        [ 1.6280e-02,  1.1553e-02,  1.5733e-02,  1.2952e-02, -5.0924e-02,\n",
       "         -4.5694e-02, -5.7350e-02, -8.5702e-02,  4.2825e-02,  3.7737e-03,\n",
       "         -4.3730e-02, -2.3120e-02,  1.7296e-02,  1.6149e-02, -4.7720e-02,\n",
       "          3.5691e-02,  3.5936e-02, -3.4023e-03, -5.2664e-02,  2.8396e-02],\n",
       "        [-5.3504e-03,  1.1426e-02, -1.1699e-02, -1.9073e-02,  1.1979e-02,\n",
       "          3.0939e-02,  1.3163e-02,  1.5407e-02, -1.1997e-02, -5.1190e-03,\n",
       "          3.8705e-02,  1.6560e-02, -1.6944e-02, -2.1004e-02, -9.0740e-03,\n",
       "          7.3429e-04, -1.4154e-02, -1.3796e-02,  2.0441e-02, -2.0984e-02],\n",
       "        [ 2.2742e-02, -5.1159e-03,  2.9387e-02,  1.5977e-02, -1.4042e-02,\n",
       "          6.4622e-03, -1.2822e-02, -1.2438e-02,  2.6413e-02, -4.2985e-03,\n",
       "         -6.1466e-03, -5.7989e-03, -3.5555e-03,  4.4417e-03, -2.7306e-03,\n",
       "          8.4444e-03,  2.1335e-02, -9.7454e-03, -1.6639e-02,  2.0539e-02],\n",
       "        [-1.1709e-02,  2.0617e-02, -2.9485e-02, -4.1661e-02,  3.4691e-02,\n",
       "          6.2922e-02,  3.0163e-02,  1.9055e-02, -1.0732e-02, -6.6769e-04,\n",
       "          9.5338e-02,  4.3415e-02, -5.2969e-02, -4.9028e-02, -2.3107e-02,\n",
       "          1.5457e-03, -2.4447e-02, -2.7901e-02,  3.8412e-02, -5.8718e-02],\n",
       "        [-1.3101e-02,  1.7499e-02,  1.4434e-02,  1.4309e-02, -6.9651e-03,\n",
       "          2.9574e-02,  2.7654e-02, -3.3145e-03, -1.0469e-02,  3.1505e-02,\n",
       "          6.8181e-03, -9.8184e-04,  6.2912e-03,  2.7894e-03, -3.4908e-03,\n",
       "          2.4189e-02, -8.0546e-03, -3.3987e-03, -6.0427e-03, -8.0672e-03],\n",
       "        [ 1.4406e-05, -2.6104e-03,  3.4579e-03, -1.3574e-02,  1.0070e-02,\n",
       "          1.6510e-02,  3.1521e-03,  1.8733e-02, -1.8516e-03,  1.8164e-03,\n",
       "         -8.2018e-03,  7.2168e-03, -1.8295e-02, -2.1456e-02,  8.2311e-03,\n",
       "         -1.0669e-02, -8.2123e-04, -7.3278e-03, -1.2039e-06, -1.5251e-02],\n",
       "        [ 1.8477e-02,  1.9457e-02, -4.4007e-03, -2.4648e-02, -5.8404e-03,\n",
       "          8.4221e-03,  7.5648e-03,  1.0110e-02, -3.4947e-03, -7.6810e-03,\n",
       "          2.0077e-03,  1.7765e-02, -2.4554e-02, -2.9808e-03,  7.2008e-03,\n",
       "          7.8563e-03, -1.2894e-02, -4.8773e-03,  8.8380e-03, -1.5299e-02],\n",
       "        [ 8.8892e-03, -1.7669e-02, -1.8681e-02, -1.1560e-02,  1.9075e-02,\n",
       "         -4.5692e-03, -4.2252e-04,  2.7437e-02,  2.0250e-02, -8.0630e-03,\n",
       "          4.0890e-02,  6.2086e-03,  1.5323e-03,  1.0799e-02,  2.2552e-02,\n",
       "         -1.8575e-02,  6.4917e-04, -1.6839e-02,  7.9576e-03,  2.1278e-02],\n",
       "        [-3.1593e-02, -5.6603e-02,  1.2741e-02,  4.0861e-02,  4.3606e-02,\n",
       "         -1.2496e-02,  1.8466e-02,  6.2446e-03,  7.9114e-03, -2.5504e-03,\n",
       "          6.2515e-04, -2.0707e-02,  3.2911e-02,  3.6597e-03,  9.5953e-03,\n",
       "         -3.6714e-02,  1.4290e-02,  2.0538e-02,  9.9849e-03,  2.5001e-02],\n",
       "        [ 3.2201e-02,  4.8792e-02, -1.6014e-02, -6.2418e-02, -1.0217e-02,\n",
       "          3.5922e-02,  1.2117e-02,  7.2828e-03, -1.6562e-03, -2.1853e-02,\n",
       "          4.0132e-02,  4.8867e-02, -6.1651e-02, -2.2190e-02, -1.2673e-02,\n",
       "          2.4132e-02, -2.6596e-02, -2.3533e-02,  2.5166e-02, -4.3040e-02],\n",
       "        [ 2.8549e-02,  2.6469e-02,  2.0736e-03, -1.1990e-02, -8.3049e-03,\n",
       "         -4.0363e-03,  3.1373e-03, -2.9732e-02,  7.9896e-03,  2.6227e-02,\n",
       "         -2.3298e-02, -1.4690e-02, -2.2712e-02, -2.1423e-02, -1.1903e-02,\n",
       "          3.1885e-02, -3.7694e-03, -1.7093e-02, -2.1069e-02, -1.9594e-02],\n",
       "        [ 2.5501e-02,  3.6535e-02, -1.5087e-02, -6.1441e-02,  1.3355e-02,\n",
       "          5.6587e-02,  2.2472e-02,  1.5173e-02,  5.2668e-03, -1.5070e-02,\n",
       "          6.5883e-02,  5.5121e-02, -7.7963e-02, -4.3189e-02, -1.3837e-02,\n",
       "          1.3461e-02, -2.2592e-02, -3.1511e-02,  3.0201e-02, -5.7461e-02],\n",
       "        [-5.3663e-02,  8.8189e-02,  2.9308e-02, -5.3698e-03, -9.3011e-02,\n",
       "          2.3904e-02, -1.3581e-02, -8.7273e-02, -6.0683e-02,  2.0870e-02,\n",
       "         -9.3175e-02, -3.4863e-03,  3.2907e-02, -1.1477e-02, -9.6523e-02,\n",
       "          8.2694e-02, -1.7937e-02,  2.0099e-02, -2.8894e-02, -3.7022e-02],\n",
       "        [ 2.6355e-02, -1.6686e-02, -9.6795e-03, -4.1144e-03,  1.9113e-02,\n",
       "         -9.7362e-03, -6.4107e-03,  2.4212e-02,  7.7968e-03, -1.3184e-02,\n",
       "          1.3448e-02, -1.3351e-03, -1.8565e-02, -5.6738e-03,  2.4709e-02,\n",
       "         -2.1746e-02,  3.2851e-03, -4.9403e-03,  9.2165e-03,  1.4463e-03],\n",
       "        [-1.3763e-02, -2.5611e-02,  3.7825e-03,  2.5389e-02,  1.4081e-02,\n",
       "         -1.0464e-02, -1.5083e-04, -1.6952e-03,  9.2047e-03,  1.2036e-02,\n",
       "          1.1050e-03, -1.7429e-02,  2.1480e-02,  6.6703e-03,  5.8435e-03,\n",
       "         -1.2575e-02,  1.2558e-02,  4.2041e-03, -8.6833e-03,  1.7711e-02],\n",
       "        [-8.8059e-05, -1.2264e-02,  6.2605e-03,  1.0201e-02, -1.9939e-03,\n",
       "         -1.3736e-02, -1.8725e-03,  7.3553e-03,  4.0088e-03, -2.9585e-03,\n",
       "         -1.5972e-02,  4.4812e-04,  6.9964e-03,  1.8527e-02,  1.7768e-02,\n",
       "         -1.0732e-02,  5.8376e-03,  1.3978e-02, -3.3951e-03,  1.1593e-02],\n",
       "        [ 1.9098e-02,  1.3587e-02, -1.8033e-02, -3.7407e-02,  1.3644e-02,\n",
       "          1.8165e-02,  1.9469e-02,  1.5241e-02,  1.5957e-02, -8.8886e-03,\n",
       "          5.1091e-02,  2.7223e-02, -2.8481e-02, -4.8674e-03,  5.2312e-03,\n",
       "          6.1031e-03, -1.6191e-02, -2.4431e-02,  1.8686e-02, -8.3081e-03],\n",
       "        [-3.1826e-02,  1.3974e-02, -2.8844e-02,  8.2040e-04,  3.0856e-02,\n",
       "          1.5783e-02,  5.6670e-02,  2.4309e-02, -4.0811e-02,  5.5489e-02,\n",
       "          1.9679e-02,  4.9840e-03, -8.0314e-03, -5.5026e-03,  2.4732e-02,\n",
       "          3.6337e-03, -3.5265e-02,  1.3957e-02,  1.3667e-02, -4.9359e-02],\n",
       "        [-4.1230e-03,  3.6701e-02,  1.1337e-02, -3.3591e-02, -6.8061e-02,\n",
       "         -2.5772e-02, -6.9330e-02, -8.5539e-02,  1.0801e-02, -4.9741e-02,\n",
       "         -6.2402e-02, -1.9646e-02,  4.1514e-02, -1.4794e-02, -9.7987e-02,\n",
       "          4.3227e-02,  1.2837e-02, -2.1307e-02, -2.4594e-02,  3.6329e-02],\n",
       "        [ 2.0507e-02, -4.9685e-03,  1.5204e-02,  3.1469e-02, -5.8492e-03,\n",
       "          1.4924e-02,  7.2864e-03,  2.8069e-02, -2.8868e-03,  4.6060e-04,\n",
       "          2.6877e-02, -7.8288e-03,  1.0877e-02,  2.5700e-02,  2.8204e-02,\n",
       "          8.1754e-04,  3.5427e-03, -3.0246e-03,  5.8554e-03,  2.5778e-02]],\n",
       "       grad_fn=<TBackward0>)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_grad=grad(train_loss,learner.layers[0].weight,retain_graph=True,\n",
    "                                 create_graph=True,\n",
    "                                 allow_unused=True)\n",
    "train_grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYzYDg3h1dhW"
   },
   "source": [
    "Next we ADAPT the learner by taking one step on the CLONED parameters in direction of the gradient of the TRAINING loss above. This is the part that the l2l libarary does for us as per the MAML algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "AJvtE00Q1dhW"
   },
   "outputs": [],
   "source": [
    "learner.adapt(train_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JtRamppW1dhW"
   },
   "source": [
    "We can check what has happended:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "id": "01d8vOOg1dhW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1093, -0.0090, -0.1264,  0.0577,  0.0315, -0.0519, -0.1989,  0.2100,\n",
       "          0.0962,  0.0178, -0.0338,  0.0737, -0.0236,  0.1632, -0.0033,  0.1727,\n",
       "          0.0513, -0.0814,  0.1097,  0.0720],\n",
       "        [ 0.0081, -0.0405, -0.0318, -0.0756, -0.0192, -0.0874,  0.1203, -0.0057,\n",
       "          0.1403,  0.0180,  0.1108,  0.1088, -0.0294, -0.1209,  0.1341,  0.1901,\n",
       "          0.0245,  0.2196, -0.0210,  0.1682],\n",
       "        [ 0.0638,  0.0831,  0.0521, -0.0859, -0.1938,  0.2153,  0.0952, -0.2169,\n",
       "          0.1700,  0.2111,  0.1200, -0.0361, -0.1854,  0.0802, -0.1345, -0.0675,\n",
       "          0.0600, -0.0317, -0.2145, -0.0467],\n",
       "        [ 0.0693, -0.0078, -0.0422,  0.0707, -0.0363,  0.1351,  0.0119,  0.1564,\n",
       "          0.0597, -0.1172, -0.1517, -0.0616,  0.1465,  0.1363,  0.1599, -0.1625,\n",
       "          0.0481, -0.1164,  0.0554, -0.0715],\n",
       "        [-0.0314,  0.1199, -0.0061, -0.2253,  0.1160,  0.0036,  0.2146,  0.0978,\n",
       "         -0.0257,  0.1206, -0.1717, -0.1101,  0.0444, -0.0624,  0.1906,  0.0504,\n",
       "         -0.1720, -0.1568,  0.0523,  0.0390],\n",
       "        [ 0.1083,  0.0279,  0.1211,  0.1529,  0.1814, -0.0350,  0.2111, -0.0946,\n",
       "         -0.0866, -0.1005,  0.0950, -0.0891,  0.0582,  0.0446, -0.0896,  0.1790,\n",
       "         -0.1742,  0.1758, -0.0172,  0.0745],\n",
       "        [-0.0579,  0.2255,  0.1095, -0.2173, -0.0315,  0.0706, -0.1010, -0.1239,\n",
       "          0.1897,  0.0943, -0.1443,  0.1519,  0.0423, -0.1729,  0.0081, -0.0537,\n",
       "          0.2174, -0.0638,  0.1553,  0.0918],\n",
       "        [ 0.0938, -0.0823,  0.0313,  0.1974, -0.0229, -0.1203, -0.1001,  0.1703,\n",
       "         -0.2208, -0.1354,  0.1833,  0.0916,  0.1354, -0.1599, -0.1745, -0.2127,\n",
       "         -0.1207,  0.0702,  0.1735, -0.2105],\n",
       "        [ 0.0641,  0.1231, -0.0414, -0.1852,  0.1664, -0.0830,  0.1426, -0.0361,\n",
       "          0.0642,  0.0466,  0.0049,  0.1549,  0.1707, -0.1449,  0.2145,  0.0504,\n",
       "          0.0023,  0.1179, -0.0512, -0.1414],\n",
       "        [ 0.0785,  0.0755, -0.1962, -0.0281, -0.0828,  0.0284, -0.1511, -0.2022,\n",
       "         -0.1419,  0.0208, -0.1939, -0.0115, -0.1490,  0.2101, -0.0271, -0.1194,\n",
       "          0.1178, -0.0979, -0.0519, -0.0603],\n",
       "        [ 0.0971,  0.0805,  0.1704,  0.1692,  0.1889,  0.0304,  0.1221, -0.1752,\n",
       "         -0.0297, -0.0950,  0.1168, -0.1566, -0.1935, -0.1671, -0.0611, -0.0354,\n",
       "         -0.0297, -0.2212, -0.0998,  0.1211],\n",
       "        [-0.0491, -0.1393, -0.0536,  0.0264,  0.0426,  0.1229,  0.0026, -0.0783,\n",
       "         -0.0595, -0.0708, -0.0402,  0.0565,  0.1317,  0.1423, -0.0663, -0.1800,\n",
       "          0.0833, -0.1377, -0.1234,  0.0044],\n",
       "        [-0.1639,  0.0833, -0.1124,  0.1117, -0.1579, -0.1834,  0.0632, -0.1655,\n",
       "         -0.0803, -0.0930, -0.2007,  0.2193,  0.1167, -0.0266, -0.1338, -0.0346,\n",
       "          0.1036,  0.1410, -0.1842,  0.1718],\n",
       "        [ 0.0047,  0.1362, -0.0230,  0.1674,  0.0069, -0.0530, -0.1314,  0.0941,\n",
       "         -0.0076,  0.1489, -0.0016, -0.0102, -0.1420,  0.1008, -0.0675, -0.1929,\n",
       "         -0.0701,  0.2120, -0.0501, -0.2119],\n",
       "        [ 0.0748,  0.0222,  0.1874,  0.1553, -0.1776,  0.0775, -0.1497,  0.1438,\n",
       "          0.1476,  0.0519, -0.0513, -0.2213,  0.0116,  0.1775,  0.1686,  0.0636,\n",
       "         -0.0550,  0.1263,  0.1943, -0.1395],\n",
       "        [ 0.0020,  0.0731,  0.1501,  0.1786, -0.0068, -0.0081,  0.0006,  0.0735,\n",
       "          0.1125,  0.1545, -0.0286,  0.1981, -0.0016, -0.1019,  0.1435, -0.0005,\n",
       "          0.1146, -0.0283, -0.0003, -0.0473],\n",
       "        [ 0.1111, -0.1409,  0.0674, -0.1241,  0.0540, -0.0934,  0.0128,  0.0976,\n",
       "          0.0659, -0.0878, -0.0762,  0.0102, -0.0475, -0.0212,  0.1553,  0.0602,\n",
       "          0.2101,  0.0613, -0.0089, -0.0216],\n",
       "        [-0.0680, -0.2203,  0.1424,  0.0415,  0.1427,  0.1100, -0.0982, -0.1891,\n",
       "         -0.0930,  0.0580, -0.1679, -0.0867,  0.1314, -0.1276,  0.0119,  0.0797,\n",
       "          0.0829,  0.1940, -0.2003,  0.1168],\n",
       "        [-0.2066,  0.0625,  0.0994, -0.0448, -0.1949,  0.0147, -0.1827, -0.1000,\n",
       "          0.2175, -0.0117, -0.1627,  0.1358,  0.0019,  0.1785,  0.1938,  0.0723,\n",
       "         -0.2215, -0.0997,  0.1583, -0.0950],\n",
       "        [ 0.1600, -0.2146, -0.1139,  0.0304,  0.2075,  0.0452,  0.2099,  0.0594,\n",
       "         -0.1863,  0.1271, -0.0809,  0.0415,  0.2016,  0.0995,  0.1421, -0.0528,\n",
       "          0.2172, -0.0837,  0.1491,  0.0136],\n",
       "        [ 0.1098,  0.0521, -0.0716,  0.1464,  0.0854, -0.2147, -0.0956, -0.2146,\n",
       "         -0.0863, -0.0599, -0.1988,  0.0063, -0.2071, -0.1734,  0.0791,  0.1300,\n",
       "          0.0252,  0.0058,  0.0861, -0.0488],\n",
       "        [ 0.0307, -0.0197,  0.0273,  0.1223,  0.1930, -0.0619, -0.1683, -0.0207,\n",
       "         -0.1164,  0.2107,  0.0642,  0.1019, -0.2007,  0.1690, -0.2061, -0.0417,\n",
       "         -0.1380,  0.0636,  0.1301, -0.2171],\n",
       "        [ 0.0789,  0.1468, -0.2220, -0.0642, -0.1130, -0.1342, -0.1558,  0.1594,\n",
       "          0.0710, -0.0060, -0.0837, -0.1206,  0.1947, -0.0298,  0.2020,  0.1816,\n",
       "          0.1726,  0.1897,  0.0420,  0.1102],\n",
       "        [-0.1561, -0.0955,  0.1048, -0.0924,  0.0869, -0.1109,  0.0681, -0.0646,\n",
       "         -0.0743, -0.1082,  0.1575,  0.1400, -0.1206,  0.2161, -0.2211,  0.1816,\n",
       "          0.1564, -0.0705, -0.0984, -0.1945],\n",
       "        [ 0.1824, -0.1868, -0.0832, -0.2103, -0.1750,  0.1629, -0.0134, -0.0181,\n",
       "          0.0719,  0.1501,  0.1663,  0.0541,  0.1693,  0.1691, -0.1115, -0.0392,\n",
       "          0.2233, -0.1573,  0.2207, -0.0219],\n",
       "        [-0.0648,  0.0623, -0.0995,  0.0740,  0.0105, -0.1554, -0.2155,  0.0254,\n",
       "         -0.1819,  0.2141,  0.1898, -0.1996,  0.1779,  0.0850,  0.0969, -0.1357,\n",
       "          0.2143,  0.1785, -0.0974, -0.0843],\n",
       "        [-0.0064,  0.2108, -0.0264, -0.0897,  0.1899,  0.0208,  0.1830,  0.0538,\n",
       "          0.1986, -0.0213,  0.1399, -0.0174, -0.0880, -0.1381,  0.2176, -0.0208,\n",
       "          0.1893,  0.1729, -0.0329, -0.2076],\n",
       "        [-0.0753,  0.1994,  0.2096,  0.1215, -0.1872, -0.1688,  0.0735, -0.0341,\n",
       "         -0.1139,  0.1632, -0.0241,  0.0373, -0.1769, -0.1069,  0.0974,  0.1193,\n",
       "          0.1526, -0.2174,  0.0444, -0.1527],\n",
       "        [-0.0969,  0.1190,  0.2158, -0.1677,  0.1449,  0.2070,  0.1884,  0.1986,\n",
       "         -0.1535, -0.1118,  0.2050, -0.1212, -0.2168,  0.0276, -0.0309, -0.1282,\n",
       "         -0.0624,  0.2122,  0.1978, -0.0667],\n",
       "        [ 0.1327, -0.0325,  0.1037,  0.1051, -0.0471,  0.2026, -0.2030, -0.0987,\n",
       "          0.0806, -0.1188,  0.1297,  0.0481,  0.1658,  0.2149,  0.0360, -0.1165,\n",
       "          0.0284,  0.1832, -0.1753, -0.2119],\n",
       "        [-0.1056, -0.0746, -0.0597, -0.0153, -0.1044,  0.0348, -0.1229, -0.1141,\n",
       "          0.0615,  0.1224, -0.0002, -0.0720,  0.2026, -0.0155, -0.1387, -0.0081,\n",
       "         -0.0560, -0.1524, -0.1610, -0.1680],\n",
       "        [ 0.1519, -0.0068,  0.1084, -0.0495,  0.1219,  0.1177,  0.1780,  0.0869,\n",
       "          0.0494, -0.0896, -0.1788,  0.0252, -0.1752, -0.2149, -0.1326, -0.0410,\n",
       "         -0.1834,  0.0869,  0.0417,  0.0535]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "id": "jSl4YwJd1dhW",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.0998, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.0990, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000],\n",
       "        [0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
       "         0.1000, 0.1000]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(net.layers[0].weight - learner.layers[0].weight)/train_grad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-HgVlYZU1dhX"
   },
   "source": [
    "So one step in the diretion of the gradient (w.r.t train_loss) has been taken. Next we compute the loss of this ADAPTED learner w.r.t. the TEST data of the task, i.e., d_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "-Pp5a0c11dhX"
   },
   "outputs": [],
   "source": [
    "test_preds = learner(d_test[0])\n",
    "adapt_loss = lossfn(test_preds,d_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lQoWmk3z1dhX"
   },
   "source": [
    "The main MAML update to the original network net takes place now, by back-propagating through the (cumulative) adaptation loss (across possibly many tasks, here there was just one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "p-5D0P_U1dhX"
   },
   "outputs": [],
   "source": [
    "task_count = 1\n",
    "optimizer.zero_grad()\n",
    "total_loss = adapt_loss/task_count\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "id": "j3yPD1TS1dhX",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1121, -0.0092, -0.1242,  0.0586,  0.0297, -0.0524, -0.2009,  0.2082,\n",
       "          0.0989,  0.0167, -0.0350,  0.0734, -0.0245,  0.1639, -0.0037,  0.1735,\n",
       "          0.0533, -0.0820,  0.1081,  0.0737],\n",
       "        [ 0.0070, -0.0386, -0.0314, -0.0761, -0.0223, -0.0870,  0.1188, -0.0043,\n",
       "          0.1373,  0.0178,  0.1070,  0.1092, -0.0285, -0.1202,  0.1345,  0.1904,\n",
       "          0.0238,  0.2207, -0.0216,  0.1676],\n",
       "        [ 0.0640,  0.0818,  0.0549, -0.0860, -0.1960,  0.2120,  0.0915, -0.2207,\n",
       "          0.1724,  0.2092,  0.1118, -0.0383, -0.1833,  0.0796, -0.1369, -0.0677,\n",
       "          0.0626, -0.0314, -0.2176, -0.0437],\n",
       "        [ 0.0673, -0.0091, -0.0399,  0.0687, -0.0365,  0.1350,  0.0106,  0.1543,\n",
       "          0.0619, -0.1191, -0.1569, -0.0604,  0.1461,  0.1343,  0.1576, -0.1636,\n",
       "          0.0497, -0.1160,  0.0541, -0.0714],\n",
       "        [-0.0323,  0.1176, -0.0047, -0.2212,  0.1174,  0.0010,  0.2144,  0.0933,\n",
       "         -0.0245,  0.1226, -0.1737, -0.1152,  0.0480, -0.0632,  0.1881,  0.0511,\n",
       "         -0.1703, -0.1568,  0.0504,  0.0414],\n",
       "        [ 0.1063,  0.0285,  0.1204,  0.1526,  0.1802, -0.0356,  0.2101, -0.0953,\n",
       "         -0.0884, -0.1013,  0.0938, -0.0885,  0.0592,  0.0450, -0.0909,  0.1788,\n",
       "         -0.1746,  0.1772, -0.0168,  0.0737],\n",
       "        [-0.0592,  0.2223,  0.1121, -0.2142, -0.0324,  0.0700, -0.1042, -0.1266,\n",
       "          0.1930,  0.0924, -0.1438,  0.1512,  0.0449, -0.1716,  0.0058, -0.0548,\n",
       "          0.2211, -0.0635,  0.1538,  0.0957],\n",
       "        [ 0.0972, -0.0804,  0.0301,  0.1933, -0.0204, -0.1160, -0.0979,  0.1716,\n",
       "         -0.2188, -0.1354,  0.1900,  0.0960,  0.1273, -0.1636, -0.1743, -0.2121,\n",
       "         -0.1217,  0.0676,  0.1755, -0.2158],\n",
       "        [ 0.0651,  0.1235, -0.0408, -0.1848,  0.1666, -0.0847,  0.1439, -0.0381,\n",
       "          0.0637,  0.0457,  0.0026,  0.1533,  0.1718, -0.1449,  0.2135,  0.0516,\n",
       "          0.0014,  0.1185, -0.0504, -0.1407],\n",
       "        [ 0.0772,  0.0727, -0.1929, -0.0257, -0.0850,  0.0247, -0.1555, -0.2088,\n",
       "         -0.1368,  0.0200, -0.1994, -0.0129, -0.1466,  0.2112, -0.0307, -0.1194,\n",
       "          0.1225, -0.0967, -0.0563, -0.0568],\n",
       "        [ 0.0977,  0.0827,  0.1660,  0.1687,  0.1922,  0.0298,  0.1275, -0.1756,\n",
       "         -0.0317, -0.0921,  0.1230, -0.1583, -0.1927, -0.1669, -0.0611, -0.0327,\n",
       "         -0.0338, -0.2225, -0.0970,  0.1200],\n",
       "        [-0.0539, -0.1353, -0.0497,  0.0277,  0.0377,  0.1261,  0.0028, -0.0833,\n",
       "         -0.0626, -0.0685, -0.0467,  0.0568,  0.1332,  0.1412, -0.0718, -0.1758,\n",
       "          0.0834, -0.1357, -0.1259,  0.0016],\n",
       "        [-0.1623,  0.0845, -0.1108,  0.1130, -0.1630, -0.1880,  0.0575, -0.1740,\n",
       "         -0.0760, -0.0927, -0.2050,  0.2170,  0.1184, -0.0250, -0.1385, -0.0311,\n",
       "          0.1072,  0.1406, -0.1894,  0.1746],\n",
       "        [ 0.0042,  0.1373, -0.0241,  0.1655,  0.0081, -0.0499, -0.1301,  0.0956,\n",
       "         -0.0088,  0.1484,  0.0023, -0.0085, -0.1437,  0.0987, -0.0684, -0.1928,\n",
       "         -0.0715,  0.2107, -0.0481, -0.2140],\n",
       "        [ 0.0771,  0.0217,  0.1903,  0.1569, -0.1790,  0.0782, -0.1510,  0.1426,\n",
       "          0.1502,  0.0515, -0.0519, -0.2219,  0.0113,  0.1779,  0.1683,  0.0644,\n",
       "         -0.0529,  0.1253,  0.1926, -0.1375],\n",
       "        [ 0.0009,  0.0751,  0.1472,  0.1744, -0.0034, -0.0018,  0.0036,  0.0754,\n",
       "          0.1114,  0.1544, -0.0190,  0.2025, -0.0069, -0.1068,  0.1411, -0.0003,\n",
       "          0.1121, -0.0311,  0.0036, -0.0532],\n",
       "        [ 0.1098, -0.1391,  0.0688, -0.1226,  0.0533, -0.0904,  0.0155,  0.0973,\n",
       "          0.0648, -0.0846, -0.0755,  0.0101, -0.0468, -0.0210,  0.1550,  0.0626,\n",
       "          0.2093,  0.0609, -0.0095, -0.0225],\n",
       "        [-0.0680, -0.2206,  0.1427,  0.0402,  0.1437,  0.1117, -0.0978, -0.1872,\n",
       "         -0.0932,  0.0581, -0.1687, -0.0860,  0.1296, -0.1298,  0.0127,  0.0786,\n",
       "          0.0828,  0.1933, -0.2003,  0.1153],\n",
       "        [-0.2047,  0.0644,  0.0990, -0.0473, -0.1955,  0.0155, -0.1819, -0.0990,\n",
       "          0.2172, -0.0125, -0.1625,  0.1375, -0.0006,  0.1782,  0.1945,  0.0731,\n",
       "         -0.2228, -0.1001,  0.1592, -0.0965],\n",
       "        [ 0.1608, -0.2164, -0.1158,  0.0293,  0.2094,  0.0447,  0.2099,  0.0621,\n",
       "         -0.1843,  0.1263, -0.0768,  0.0422,  0.2017,  0.1006,  0.1443, -0.0546,\n",
       "          0.2172, -0.0854,  0.1499,  0.0157],\n",
       "        [ 0.1066,  0.0464, -0.0703,  0.1504,  0.0898, -0.2159, -0.0937, -0.2140,\n",
       "         -0.0855, -0.0602, -0.1987,  0.0042, -0.2038, -0.1731,  0.0800,  0.1264,\n",
       "          0.0266,  0.0079,  0.0871, -0.0463],\n",
       "        [ 0.0339, -0.0149,  0.0257,  0.1161,  0.1920, -0.0583, -0.1670, -0.0199,\n",
       "         -0.1166,  0.2085,  0.0682,  0.1068, -0.2068,  0.1667, -0.2074, -0.0393,\n",
       "         -0.1407,  0.0613,  0.1326, -0.2214],\n",
       "        [ 0.0817,  0.1494, -0.2218, -0.0654, -0.1138, -0.1346, -0.1555,  0.1564,\n",
       "          0.0718, -0.0034, -0.0860, -0.1221,  0.1924, -0.0319,  0.2008,  0.1847,\n",
       "          0.1722,  0.1880,  0.0399,  0.1082],\n",
       "        [-0.1535, -0.0918,  0.1032, -0.0986,  0.0883, -0.1052,  0.0704, -0.0631,\n",
       "         -0.0738, -0.1097,  0.1640,  0.1455, -0.1284,  0.2118, -0.2225,  0.1830,\n",
       "          0.1541, -0.0737, -0.0954, -0.2002],\n",
       "        [ 0.1770, -0.1780, -0.0802, -0.2109, -0.1843,  0.1652, -0.0148, -0.0268,\n",
       "          0.0658,  0.1521,  0.1570,  0.0538,  0.1726,  0.1680, -0.1211, -0.0309,\n",
       "          0.2215, -0.1553,  0.2178, -0.0256],\n",
       "        [-0.0621,  0.0606, -0.1004,  0.0736,  0.0125, -0.1564, -0.2161,  0.0278,\n",
       "         -0.1811,  0.2128,  0.1911, -0.1997,  0.1761,  0.0845,  0.0994, -0.1379,\n",
       "          0.2146,  0.1780, -0.0965, -0.0842],\n",
       "        [-0.0078,  0.2082, -0.0260, -0.0872,  0.1913,  0.0197,  0.1830,  0.0537,\n",
       "          0.1995, -0.0201,  0.1400, -0.0192, -0.0859, -0.1375,  0.2182, -0.0220,\n",
       "          0.1905,  0.1733, -0.0338, -0.2059],\n",
       "        [-0.0753,  0.1981,  0.2103,  0.1225, -0.1874, -0.1702,  0.0733, -0.0333,\n",
       "         -0.1135,  0.1629, -0.0257,  0.0373, -0.1762, -0.1050,  0.0992,  0.1183,\n",
       "          0.1532, -0.2160,  0.0441, -0.1516],\n",
       "        [-0.0950,  0.1203,  0.2140, -0.1714,  0.1462,  0.2088,  0.1904,  0.2001,\n",
       "         -0.1519, -0.1127,  0.2101, -0.1185, -0.2197,  0.0271, -0.0304, -0.1276,\n",
       "         -0.0640,  0.2098,  0.1996, -0.0675],\n",
       "        [ 0.1295, -0.0311,  0.1008,  0.1051, -0.0440,  0.2042, -0.1974, -0.0962,\n",
       "          0.0765, -0.1132,  0.1317,  0.0486,  0.1649,  0.2143,  0.0385, -0.1161,\n",
       "          0.0249,  0.1846, -0.1740, -0.2168],\n",
       "        [-0.1060, -0.0709, -0.0586, -0.0186, -0.1112,  0.0322, -0.1299, -0.1227,\n",
       "          0.0626,  0.1174, -0.0065, -0.0740,  0.2067, -0.0169, -0.1485, -0.0037,\n",
       "         -0.0547, -0.1546, -0.1635, -0.1644],\n",
       "        [ 0.1539, -0.0073,  0.1099, -0.0464,  0.1213,  0.1192,  0.1787,  0.0897,\n",
       "          0.0491, -0.0895, -0.1761,  0.0244, -0.1741, -0.2123, -0.1298, -0.0409,\n",
       "         -0.1830,  0.0866,  0.0423,  0.0561]], requires_grad=True)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "zdq8ERqb1dhX"
   },
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "e74Q_2eH1dhX",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 1.1105e-01, -8.1686e-03, -1.2519e-01,  5.9588e-02,  3.0707e-02,\n",
       "         -5.3418e-02, -1.9991e-01,  2.0925e-01,  9.7909e-02,  1.5700e-02,\n",
       "         -3.3958e-02,  7.2418e-02, -2.5471e-02,  1.6286e-01, -2.7217e-03,\n",
       "          1.7254e-01,  5.4342e-02, -8.2958e-02,  1.0910e-01,  7.4745e-02],\n",
       "        [ 6.0320e-03, -3.7642e-02, -3.2428e-02, -7.5061e-02, -2.1295e-02,\n",
       "         -8.8029e-02,  1.1975e-01, -3.2606e-03,  1.3830e-01,  1.6785e-02,\n",
       "          1.0801e-01,  1.0817e-01, -2.9536e-02, -1.2119e-01,  1.3553e-01,\n",
       "          1.8938e-01,  2.4835e-02,  2.1970e-01, -2.0576e-02,  1.6860e-01],\n",
       "        [ 6.4964e-02,  8.2770e-02,  5.5930e-02, -8.4968e-02, -1.9503e-01,\n",
       "          2.1103e-01,  9.0497e-02, -2.1967e-01,  1.7340e-01,  2.1016e-01,\n",
       "          1.1075e-01, -3.9335e-02, -1.8227e-01,  7.8630e-02, -1.3587e-01,\n",
       "         -6.6652e-02,  6.1624e-02, -3.2398e-02, -2.1863e-01, -4.2700e-02],\n",
       "        [ 6.6347e-02, -8.0743e-03, -4.0870e-02,  6.9673e-02, -3.5518e-02,\n",
       "          1.3397e-01,  1.1622e-02,  1.5535e-01,  6.0894e-02, -1.2006e-01,\n",
       "         -1.5595e-01, -6.1398e-02,  1.4508e-01,  1.3329e-01,  1.5859e-01,\n",
       "         -1.6457e-01,  5.0690e-02, -1.1703e-01,  5.5101e-02, -7.0447e-02],\n",
       "        [-3.3277e-02,  1.1658e-01, -5.7360e-03, -2.2216e-01,  1.1638e-01,\n",
       "          1.9995e-03,  2.1345e-01,  9.4288e-02, -2.5455e-02,  1.2157e-01,\n",
       "         -1.7275e-01, -1.1420e-01,  4.7045e-02, -6.2183e-02,  1.8709e-01,\n",
       "          5.0138e-02, -1.6931e-01, -1.5576e-01,  4.9363e-02,  4.0428e-02],\n",
       "        [ 1.0726e-01,  2.9538e-02,  1.2138e-01,  1.5364e-01,  1.8124e-01,\n",
       "         -3.6623e-02,  2.1113e-01, -9.6295e-02, -8.7366e-02, -1.0227e-01,\n",
       "          9.2817e-02, -8.9524e-02,  6.0216e-02,  4.3962e-02, -9.1905e-02,\n",
       "          1.7982e-01, -1.7558e-01,  1.7624e-01, -1.5782e-02,  7.4672e-02],\n",
       "        [-6.0172e-02,  2.2133e-01,  1.1112e-01, -2.1517e-01, -3.3413e-02,\n",
       "          6.8963e-02, -1.0524e-01, -1.2756e-01,  1.9204e-01,  9.1398e-02,\n",
       "         -1.4481e-01,  1.5220e-01,  4.5893e-02, -1.7057e-01,  4.8015e-03,\n",
       "         -5.5795e-02,  2.2214e-01, -6.2469e-02,  1.5275e-01,  9.4746e-02],\n",
       "        [ 9.6167e-02, -7.9448e-02,  3.1069e-02,  1.9234e-01, -2.1388e-02,\n",
       "         -1.1699e-01, -9.8860e-02,  1.7056e-01, -2.1783e-01, -1.3645e-01,\n",
       "          1.8901e-01,  9.6989e-02,  1.2835e-01, -1.6456e-01, -1.7529e-01,\n",
       "         -2.1313e-01, -1.2273e-01,  6.8557e-02,  1.7645e-01, -2.1479e-01],\n",
       "        [ 6.4120e-02,  1.2251e-01, -4.1831e-02, -1.8585e-01,  1.6556e-01,\n",
       "         -8.3711e-02,  1.4285e-01, -3.7095e-02,  6.2662e-02,  4.4744e-02,\n",
       "          3.5670e-03,  1.5429e-01,  1.7079e-01, -1.4390e-01,  2.1249e-01,\n",
       "          5.0599e-02,  2.3808e-03,  1.1953e-01, -4.9410e-02, -1.4168e-01],\n",
       "        [ 7.6159e-02,  7.1695e-02, -1.9385e-01, -2.6710e-02, -8.3968e-02,\n",
       "          2.3682e-02, -1.5445e-01, -2.0781e-01, -1.3781e-01,  1.8996e-02,\n",
       "         -1.9836e-01, -1.1901e-02, -1.4557e-01,  2.1018e-01, -2.9739e-02,\n",
       "         -1.2038e-01,  1.2149e-01, -9.5672e-02, -5.5305e-02, -5.5774e-02],\n",
       "        [ 9.6726e-02,  8.1653e-02,  1.6703e-01,  1.6969e-01,  1.9120e-01,\n",
       "          3.0788e-02,  1.2646e-01, -1.7658e-01, -3.0673e-02, -9.1095e-02,\n",
       "          1.2401e-01, -1.5725e-01, -1.9365e-01, -1.6591e-01, -6.2069e-02,\n",
       "         -3.3697e-02, -3.2836e-02, -2.2153e-01, -9.7952e-02,  1.1898e-01],\n",
       "        [-5.2921e-02, -1.3633e-01, -5.0653e-02,  2.8653e-02,  3.8688e-02,\n",
       "          1.2710e-01,  1.7815e-03, -8.2261e-02, -6.1610e-02, -6.9504e-02,\n",
       "         -4.5696e-02,  5.7840e-02,  1.3224e-01,  1.4218e-01, -7.0762e-02,\n",
       "         -1.7681e-01,  8.2360e-02, -1.3670e-01, -1.2485e-01,  2.6404e-03],\n",
       "        [-1.6125e-01,  8.5494e-02, -1.1179e-01,  1.1197e-01, -1.6398e-01,\n",
       "         -1.8699e-01,  5.8476e-02, -1.7305e-01, -7.7004e-02, -9.1667e-02,\n",
       "         -2.0604e-01,  2.1798e-01,  1.1940e-01, -2.4011e-02, -1.3753e-01,\n",
       "         -3.0054e-02,  1.0623e-01,  1.3961e-01, -1.9045e-01,  1.7361e-01],\n",
       "        [ 5.2143e-03,  1.3834e-01, -2.3144e-02,  1.6445e-01,  9.0625e-03,\n",
       "         -5.0938e-02, -1.3112e-01,  9.6629e-02, -9.7761e-03,  1.4741e-01,\n",
       "          1.2557e-03, -7.5093e-03, -1.4268e-01,  9.7730e-02, -6.9436e-02,\n",
       "         -1.9378e-01, -7.2528e-02,  2.1166e-01, -4.7092e-02, -2.1504e-01],\n",
       "        [ 7.8053e-02,  2.2716e-02,  1.9131e-01,  1.5588e-01, -1.7800e-01,\n",
       "          7.7175e-02, -1.4998e-01,  1.4159e-01,  1.4922e-01,  5.2458e-02,\n",
       "         -5.2937e-02, -2.2289e-01,  1.2293e-02,  1.7693e-01,  1.6732e-01,\n",
       "          6.5443e-02, -5.3852e-02,  1.2433e-01,  1.9161e-01, -1.3648e-01],\n",
       "        [-1.3661e-04,  7.4136e-02,  1.4619e-01,  1.7539e-01, -4.3504e-03,\n",
       "         -8.0422e-04,  2.6312e-03,  7.6394e-02,  1.1245e-01,  1.5342e-01,\n",
       "         -1.8045e-02,  2.0348e-01, -7.8614e-03, -1.0575e-01,  1.4215e-01,\n",
       "         -1.3120e-03,  1.1312e-01, -3.2058e-02,  4.5530e-03, -5.4218e-02],\n",
       "        [ 1.1077e-01, -1.4012e-01,  6.9805e-02, -1.2365e-01,  5.2336e-02,\n",
       "         -8.9420e-02,  1.4518e-02,  9.6285e-02,  6.5828e-02, -8.3606e-02,\n",
       "         -7.6540e-02,  1.1103e-02, -4.5835e-02, -1.9963e-02,  1.5398e-01,\n",
       "          6.1597e-02,  2.1032e-01,  6.1934e-02, -1.0527e-02, -2.3453e-02],\n",
       "        [-6.9012e-02, -2.2158e-01,  1.4372e-01,  4.1188e-02,  1.4272e-01,\n",
       "          1.1268e-01, -9.8844e-02, -1.8821e-01, -9.2179e-02,  5.7135e-02,\n",
       "         -1.6772e-01, -8.6984e-02,  1.2860e-01, -1.2879e-01,  1.1687e-02,\n",
       "          7.7638e-02,  8.3849e-02,  1.9428e-01, -2.0132e-01,  1.1625e-01],\n",
       "        [-2.0574e-01,  6.5408e-02,  9.9993e-02, -4.6307e-02, -1.9653e-01,\n",
       "          1.6535e-02, -1.8291e-01, -1.0002e-01,  2.1619e-01, -1.3487e-02,\n",
       "         -1.6353e-01,  1.3854e-01,  4.0157e-04,  1.7919e-01,  1.9348e-01,\n",
       "          7.2063e-02, -2.2176e-01, -9.9150e-02,  1.5818e-01, -9.7542e-02],\n",
       "        [ 1.5984e-01, -2.1541e-01, -1.1678e-01,  3.0265e-02,  2.1045e-01,\n",
       "          4.3727e-02,  2.1089e-01,  6.3137e-02, -1.8531e-01,  1.2531e-01,\n",
       "         -7.5776e-02,  4.1170e-02,  2.0073e-01,  9.9593e-02,  1.4535e-01,\n",
       "         -5.5645e-02,  2.1824e-01, -8.6420e-02,  1.5093e-01,  1.6687e-02],\n",
       "        [ 1.0563e-01,  4.7438e-02, -6.9293e-02,  1.5145e-01,  9.0783e-02,\n",
       "         -2.1690e-01, -9.2715e-02, -2.1499e-01, -8.4461e-02, -6.1181e-02,\n",
       "         -1.9771e-01,  3.1967e-03, -2.0480e-01, -1.7408e-01,  8.1027e-02,\n",
       "          1.2737e-01,  2.7625e-02,  6.8868e-03,  8.8133e-02, -4.5330e-02],\n",
       "        [ 3.4905e-02, -1.5855e-02,  2.4736e-02,  1.1507e-01,  1.9297e-01,\n",
       "         -5.9341e-02, -1.6804e-01, -1.8940e-02, -1.1557e-01,  2.0754e-01,\n",
       "          6.7208e-02,  1.0781e-01, -2.0583e-01,  1.6775e-01, -2.0636e-01,\n",
       "         -4.0305e-02, -1.4171e-01,  6.0253e-02,  1.3361e-01, -2.2036e-01],\n",
       "        [ 8.0705e-02,  1.4840e-01, -2.2080e-01, -6.6410e-02, -1.1480e-01,\n",
       "         -1.3363e-01, -1.5651e-01,  1.5539e-01,  7.0826e-02, -2.3811e-03,\n",
       "         -8.6997e-02, -1.2111e-01,  1.9342e-01, -3.0903e-02,  1.9982e-01,\n",
       "          1.8374e-01,  1.7325e-01,  1.8897e-01,  3.8909e-02,  1.0722e-01],\n",
       "        [-1.5253e-01, -9.2804e-02,  1.0224e-01, -9.9593e-02,  8.9277e-02,\n",
       "         -1.0421e-01,  6.9360e-02, -6.2079e-02, -7.2785e-02, -1.0869e-01,\n",
       "          1.6304e-01,  1.4649e-01, -1.2738e-01,  2.1279e-01, -2.2146e-01,\n",
       "          1.8398e-01,  1.5310e-01, -7.4666e-02, -9.4394e-02, -1.9920e-01],\n",
       "        [ 1.7602e-01, -1.7897e-01, -8.1225e-02, -2.0987e-01, -1.8333e-01,\n",
       "          1.6425e-01, -1.5780e-02, -2.5809e-02,  6.6816e-02,  1.5115e-01,\n",
       "          1.5801e-01,  5.4773e-02,  1.7355e-01,  1.6897e-01, -1.2012e-01,\n",
       "         -3.1923e-02,  2.2050e-01, -1.5633e-01,  2.1883e-01, -2.4558e-02],\n",
       "        [-6.1117e-02,  6.1604e-02, -9.9436e-02,  7.2593e-02,  1.1456e-02,\n",
       "         -1.5737e-01, -2.1714e-01,  2.6804e-02, -1.8209e-01,  2.1176e-01,\n",
       "          1.9014e-01, -2.0069e-01,  1.7708e-01,  8.3456e-02,  9.8374e-02,\n",
       "         -1.3892e-01,  2.1560e-01,  1.7701e-01, -9.7512e-02, -8.5174e-02],\n",
       "        [-8.7957e-03,  2.0925e-01, -2.6993e-02, -8.8208e-02,  1.9234e-01,\n",
       "          1.8726e-02,  1.8396e-01,  5.4670e-02,  1.9855e-01, -2.1064e-02,\n",
       "          1.4097e-01, -1.8151e-02, -8.6882e-02, -1.3845e-01,  2.1915e-01,\n",
       "         -2.3034e-02,  1.9151e-01,  1.7233e-01, -3.2810e-02, -2.0687e-01],\n",
       "        [-7.4272e-02,  1.9915e-01,  2.1127e-01,  1.2148e-01, -1.8635e-01,\n",
       "         -1.7116e-01,  7.4296e-02, -3.4335e-02, -1.1255e-01,  1.6185e-01,\n",
       "         -2.6662e-02,  3.6343e-02, -1.7520e-01, -1.0601e-01,  1.0018e-01,\n",
       "          1.1927e-01,  1.5222e-01, -2.1697e-01,  4.5102e-02, -1.5055e-01],\n",
       "        [-9.4037e-02,  1.2133e-01,  2.1303e-01, -1.7241e-01,  1.4723e-01,\n",
       "          2.0985e-01,  1.9139e-01,  2.0110e-01, -1.5289e-01, -1.1366e-01,\n",
       "          2.1109e-01, -1.1750e-01, -2.2067e-01,  2.6095e-02, -2.9414e-02,\n",
       "         -1.2664e-01, -6.4974e-02,  2.0876e-01,  2.0063e-01, -6.8527e-02],\n",
       "        [ 1.2854e-01, -3.2093e-02,  1.0181e-01,  1.0614e-01, -4.5013e-02,\n",
       "          2.0317e-01, -1.9836e-01, -9.7228e-02,  7.7524e-02, -1.1424e-01,\n",
       "          1.3268e-01,  4.7625e-02,  1.6395e-01,  2.1530e-01,  3.7470e-02,\n",
       "         -1.1709e-01,  2.5868e-02,  1.8358e-01, -1.7298e-01, -2.1580e-01],\n",
       "        [-1.0704e-01, -7.1948e-02, -5.9590e-02, -1.7642e-02, -1.1217e-01,\n",
       "          3.3192e-02, -1.2887e-01, -1.2368e-01,  6.3626e-02,  1.1640e-01,\n",
       "         -5.4675e-03, -7.4967e-02,  2.0575e-01, -1.5937e-02, -1.4749e-01,\n",
       "         -2.7487e-03, -5.3739e-02, -1.5358e-01, -1.6445e-01, -1.6341e-01],\n",
       "        [ 1.5295e-01, -6.3319e-03,  1.0889e-01, -4.7391e-02,  1.2233e-01,\n",
       "          1.1822e-01,  1.7970e-01,  9.0657e-02,  4.8062e-02, -8.8513e-02,\n",
       "         -1.7507e-01,  2.5388e-02, -1.7507e-01, -2.1334e-01, -1.2875e-01,\n",
       "         -4.1873e-02, -1.8401e-01,  8.5643e-02,  4.3289e-02,  5.5104e-02]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.layers[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhGOYJ5F1dhX"
   },
   "source": [
    "So, the original parameters have been updated by a gradient step using on all the task adaptation losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhQkNDoN1dhX"
   },
   "source": [
    "# Putting it all together: MAML Algorithm\n",
    "Now let's put all of the above in a loop - the MAML algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "id": "Ga8RuJqz1dhX"
   },
   "outputs": [],
   "source": [
    "import learn2learn as l2l\n",
    "import torch.optim as optim\n",
    "shots,ways = 5,5\n",
    "net = models.MLP(dims=[20,32,32,ways])\n",
    "maml = l2l.algorithms.MAML(net, lr=5e-3)\n",
    "# maml = l2l.algorithms.MAML(net0, lr=5e-3)\n",
    "optimizer = optim.Adam(maml.parameters(),lr=5e-4)\n",
    "lossfn = torch.nn.NLLLoss()\n",
    "meta_train_kloader=KShotLoader(meta_train_ds,shots=shots,ways=ways,num_tasks=1000,\n",
    "                               classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "J_3VNvEd1dhX"
   },
   "outputs": [],
   "source": [
    "# Number of epochs, tasks per step and number of fast_adaptation steps \n",
    "n_epochs=300\n",
    "task_count=32\n",
    "fas = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt_FkjsR1dhX"
   },
   "source": [
    "Note: In practice we use more than one gradient step for adpation, this is called 'fast adaptation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "id": "tf3v38Np1dhX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   299 Loss: 3.72539e-01 Avg Acc: 0.89000\n"
     ]
    }
   ],
   "source": [
    "epoch=0\n",
    "while epoch<n_epochs:\n",
    "    adapt_loss = 0.0\n",
    "    test_acc = 0.0\n",
    "    # Sample and train on a task\n",
    "    for task in range(task_count):\n",
    "        d_train,d_test=meta_train_kloader.get_task()\n",
    "        learner = maml.clone()\n",
    "        for fas_step in range(fas):\n",
    "            train_preds = learner(d_train[0])\n",
    "            train_loss = lossfn(train_preds,d_train[1])\n",
    "            learner.adapt(train_loss)\n",
    "        test_preds = learner(d_test[0])\n",
    "        adapt_loss += lossfn(test_preds,d_test[1])\n",
    "        learner.eval()\n",
    "        test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n",
    "        learner.train()\n",
    "        # Done with a task\n",
    "    # Update main network\n",
    "    print('Epoch  % 2d Loss: %2.5e Avg Acc: %2.5f'%(epoch,adapt_loss/task_count,test_acc/task_count))\n",
    "    display.clear_output(wait=True)\n",
    "    optimizer.zero_grad()\n",
    "    total_loss = adapt_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    epoch+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnidveAw1dhX"
   },
   "source": [
    "Now test the trained maml network and applying the adaption step to tasks sampled from the meta_test_ds dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "Rokj6qo21dhX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Acc: 0.87000\n"
     ]
    }
   ],
   "source": [
    "meta_test_kloader=KShotLoader(meta_test_ds,shots=shots,ways=ways,\n",
    "                              classes=None)\n",
    "test_acc = 0.0\n",
    "task_count = 20\n",
    "adapt_steps = 5\n",
    "maml.eval()\n",
    "# Sample and train on a task\n",
    "for task in range(task_count):\n",
    "    d_train,d_test=meta_test_kloader.get_task()\n",
    "    learner = maml.clone()\n",
    "    learner.eval()\n",
    "    for adapt_step in range(adapt_steps):\n",
    "        train_preds = learner(d_train[0])\n",
    "        train_loss = lossfn(train_preds,d_train[1])\n",
    "        learner.adapt(train_loss)\n",
    "    test_preds = learner(d_test[0])\n",
    "    test_acc += models.accuracy(learner,d_test[0],d_test[1],verbose=False)\n",
    "    # Done with a task\n",
    "#learner.train()\n",
    "print('Avg Acc: %2.5f'%(test_acc/task_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "nb2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
