{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "from tqdm.notebook import tqdm\n",
    "import import_ipynb\n",
    "import random\n",
    "from utils import MyDS\n",
    "import math, time, os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from pyarrow.parquet import ParquetFile\n",
    "import pyarrow as pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from IPython import display\n",
    "import psutil, gc\n",
    "# utils.hide_toggle('Imports 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "def get_memory_usage():\n",
    "    mem = psutil.virtual_memory()\n",
    "    return f'{mem.used / 1024 ** 3} of {mem.total / 1024 ** 3}; {mem.available / 1024 ** 3} available'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Net,X_test,y_test,verbose=True):\n",
    "    Net.eval()\n",
    "    m = X_test.shape[0]\n",
    "    y_pred = Net(X_test)\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose: print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    Net.train()\n",
    "    return accuracy\n",
    "# utils.hide_toggle('Function: accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_variable(Net,data):\n",
    "    step=0\n",
    "    acc=0\n",
    "    for (X,y) in data:\n",
    "            y_pred = Net(X)\n",
    "            step+=1\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "    a = acc/step\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainVals(Net,X_vals,y_vals,epochs=20,lr=5e-2,Loss=nn.NLLLoss(),verbose=False,device='cpu',batch_size=32):\n",
    "    #optimizer = optim.Adam(Net.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    Net.to(device)\n",
    "    N=X_vals.shape[0]\n",
    "    n_batches=int(N/batch_size)\n",
    "    for e in range(epochs):\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        start_time = time.time()\n",
    "        acc=0.0\n",
    "        for idx in range(0,N,n_batches):\n",
    "            if idx+32>1000: start,end=idx,N\n",
    "            else: start,end=idx,idx+batch_size\n",
    "            X,y=torch.tensor(X_vals[start:end],dtype=torch.float32),torch.tensor(y_vals[start:end],dtype=torch.int64)\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            Net.optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            if verbose: acc+=accuracy(Net,X,y,verbose=False)\n",
    "        end_time = time.time()\n",
    "        t = end_time-start_time\n",
    "        l = tot_loss.item()/step\n",
    "        if verbose:\n",
    "            a = acc/step\n",
    "            accs += [a]\n",
    "        losses += [l]\n",
    "        if verbose: \n",
    "            print('Epoch  % 2d Loss: %2.5e Accuracy: %2.5f Epoch Time: %2.5f' %(e,l,a,t))\n",
    "            print(get_memory_usage())\n",
    "            gc.collect()\n",
    "            display.clear_output(wait=True)\n",
    "    return Net,losses,accs\n",
    "# utils.hide_toggle('Function Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptron with ReLU non-lineartities; for classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dims=[5,3,2],task='classification',lr=1e-3,weight_decay=0):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dims=dims\n",
    "        self.n = len(self.dims)-1\n",
    "        self.task=task\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(self.n-1):\n",
    "            self.layers.append(nn.Linear(dims[i],dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        if task=='classification': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        elif task=='regression': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.Linear(dims[i+2],1))\n",
    "        else: self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr,weight_decay=weight_decay)\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return(x)\n",
    "# utils.hide_toggle('Class MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent network using RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.input_size=input_size\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        x = self.logsoft(x)\n",
    "        x=x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "        x = self.logsoft(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x\n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr,num_layers=1):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.d_model = hidden_size\n",
    "        self.linear_1 = nn.Linear(self.input_size, self.d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(self.d_model, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        #Positional Encoding\n",
    "        max_len = 25\n",
    "        position = torch.arange(25).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-np.math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(max_len, 1, self.d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        x =x + pe[:x.size(0)]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(1, 0)\n",
    "        x = x.max(1)[0]\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Net,data,epochs=20,lr=5e-2,Loss=nn.NLLLoss(),verbose=False,device='cpu',\n",
    "          val_ds=None,plot_accs=False,plot_losses=False):\n",
    "    #optimizer = optim.Adam(Net.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    val_losses=[]\n",
    "    accs = []\n",
    "    val_accL=[]\n",
    "    Net.to(device)\n",
    "    for e in range(epochs):\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        start_time = time.time()\n",
    "        acc=0.0\n",
    "        for (X,y) in data:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            Net.optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            if verbose: acc+=accuracy(Net,X,y,verbose=False)\n",
    "        end_time = time.time()\n",
    "        t = end_time-start_time\n",
    "        l = tot_loss.item()/step\n",
    "        if verbose:\n",
    "            a = acc/step\n",
    "            accs += [a]\n",
    "        losses += [l]\n",
    "        if verbose: \n",
    "            display.clear_output(wait=True)\n",
    "            print('Epoch  % 2d Loss: %2.5e Accuracy: %2.5f Epoch Time: %2.5f' %(e,l,a,t))\n",
    "        if plot_accs and val_ds is not None:\n",
    "            val_accL+=[accuracy(Net,val_ds.samples,val_ds.labels,verbose=False)]\n",
    "            plt.plot(np.array(val_accL),color='red')\n",
    "            plt.plot(np.array(accs),color='blue')\n",
    "            plt.show()\n",
    "        if plot_losses and val_ds is not None:\n",
    "            val_losses+=[Loss(Net(torch.tensor(val_ds.samples)),torch.tensor(val_ds.labels)).detach().numpy()]\n",
    "            plt.plot(val_losses,color='red')\n",
    "            plt.plot(losses,color='blue')\n",
    "            plt.show()\n",
    "    return Net,losses,accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainInner(Net,data,epochs=20,lr=5e-2,Loss=nn.NLLLoss(),verbose=False,device='cpu',\n",
    "                ds_test=None,checkpoint_path='/tmp//model.pth',optimizer=None):\n",
    "    #optimizer = optim.Adam(Net.parameters(),lr=lr)\n",
    "    if optimizer is None: optimizer=Net.optimizer\n",
    "    losses = []\n",
    "    accs = []\n",
    "    val_accs=[]\n",
    "    Net.to(device)\n",
    "    for e in range(epochs):\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        start_time = time.time()\n",
    "        acc=0.0\n",
    "        for (X,y) in data:\n",
    "            X=X.to(device)\n",
    "            y=y.to(device)\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "        end_time = time.time()\n",
    "        t = end_time-start_time\n",
    "        l = tot_loss.item()/step\n",
    "        a = acc/step\n",
    "        accs += [a]\n",
    "        losses += [l]\n",
    "        if ds_test is not None:\n",
    "            val_acc=accuracy(Net,ds_test.samples,ds_test.labels,verbose=False)\n",
    "            val_accs+=[val_acc]\n",
    "        if verbose: \n",
    "            print(f'Validation accuracy {val_acc} after epoch {e}')\n",
    "            print(f'Saving model checkpoint after epoch {e} at {checkpoint_path}')\n",
    "        if checkpoint_path is not None: torch.save(Net.state_dict(),checkpoint_path)\n",
    "        if verbose: \n",
    "            print('Epoch  % 2d Loss: %2.5e Accuracy: %2.5f Epoch Time: %2.5f' %(e,l,a,t))\n",
    "            # display.clear_output(wait=True)\n",
    "    return Net,losses,accs,val_accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMonitor():\n",
    "    def __init__(self,dir='/tmp/'):\n",
    "        self.dir=dir\n",
    "        os.system('rm /tmp/losses.csv /tmp/accs.csv /tmp/val_accs.csv')\n",
    "        self.cols=['epoch','val']\n",
    "        self.lossf=pd.DataFrame(columns=self.cols)\n",
    "        self.accf=pd.DataFrame(columns=self.cols)\n",
    "        self.val_accf=pd.DataFrame(columns=self.cols)\n",
    "    def append(self,e,lossL=[],accL=[],val_accL=[]):\n",
    "        losses=pd.DataFrame([[e,l] for l in lossL],columns=self.cols)\n",
    "        accs=pd.DataFrame([[e,l] for l in accL],columns=self.cols)\n",
    "        val_accs=pd.DataFrame([[e,l] for l in val_accL],columns=self.cols)\n",
    "        self.lossf=pd.concat([self.lossf,losses])\n",
    "        self.lossf.to_csv(self.dir+'losses.csv',index=False)\n",
    "        self.accf=pd.concat([self.accf,accs])\n",
    "        self.accf.to_csv(self.dir+'accs.csv',index=False)\n",
    "        self.val_accf=pd.concat([self.val_accf,val_accs])\n",
    "        self.val_accf.to_csv(self.dir+'val_accs.csv',index=False)\n",
    "    def plot(dir='/tmp/',curve=['losses','accs','val_accs']):\n",
    "        for c in curve:\n",
    "            df=pd.read_csv(dir+c+'.csv')\n",
    "            plt.plot(df.val.values,label=c)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainCSV(net,filename,feature_names,target='target',batch_size=32,row_batch_size=1000,epochs=20,\n",
    "                Loss=nn.NLLLoss(),verbose=True,device='cpu',txfTarget=lambda x:int(x*4),debug=False,\n",
    "                checkpoint_path='/tmp/model.pth',validation_data=None,optimizer=None,monitor=None):\n",
    "    # Shuffled data needs to be provided\n",
    "    #Batches of rows\n",
    "    ds_test=MyDS(validation_data[feature_names].values,validation_data[target].values,task='regression')\n",
    "    # ds_test.labels=torch.LongTensor([int(l*4) for l in ds_test.labels])\n",
    "    ds_test.labels=torch.LongTensor([txfTarget(l) for l in ds_test.labels])\n",
    "    for e in range(epochs):\n",
    "        row_batch_id=0\n",
    "        chunksf=pd.read_csv(filename,chunksize=row_batch_size)\n",
    "        chunksL=[]\n",
    "        for chunkf in chunksf:\n",
    "            chunksL+=[chunkf]\n",
    "        for df in chunksL[::-1]:\n",
    "            ds_train=MyDS(df[feature_names].values,df[target].values,task='regression')\n",
    "            # ds_train.labels=torch.LongTensor([int(l*4) for l in ds_train.labels])\n",
    "            ds_train.labels=torch.LongTensor([txfTarget(l) for l in ds_train.labels])\n",
    "            dsloader = torch.utils.data.DataLoader(dataset=ds_train,batch_size=batch_size,shuffle=True)        \n",
    "            net,losses,accs,val_accs=TrainInner(net,dsloader,epochs=1,verbose=False,ds_test=ds_test,\n",
    "                               Loss=Loss,device=device,checkpoint_path=None,\n",
    "                               optimizer=optimizer)\n",
    "            if monitor is not None: \n",
    "                monitor.append(e,losses,accs,val_accs)\n",
    "            row_batch_id+=1\n",
    "        if ds_test is not None: val_acc=accuracy(net,ds_test.samples,ds_test.labels,verbose=False)\n",
    "        if verbose: \n",
    "            print(f'Validation accuracy {val_acc} after epoch {e}')\n",
    "            if checkpoint_path is not None: print(f'Saving model checkpoint after epoch {e} at {checkpoint_path}')\n",
    "        if checkpoint_path is not None: torch.save(net.state_dict(),checkpoint_path)\n",
    "    if ds_test is not None: val_acc=accuracy(net,ds_test.samples,ds_test.labels,verbose=False)\n",
    "    print(f'Final validation accuracy {val_acc} after epoch {e}')\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainParquet(net,filename,feature_names,target='target',batch_size=32,row_batch_size=1000,epochs=20,\n",
    "                Loss=nn.NLLLoss(),verbose=True,device='cpu',txfTarget=lambda x:int(x*4),debug=False,\n",
    "                checkpoint_path='/tmp/model.pth',validation_data=None,optimizer=None,monitor=None):\n",
    "    # Shuffled data needs to be provided\n",
    "    ds_test=MyDS(validation_data[feature_names].values,validation_data[target].values,task='regression')\n",
    "    ds_test.labels=torch.LongTensor([txfTarget(l) for l in ds_test.labels])\n",
    "    for e in range(epochs):\n",
    "        row_batch_id=0\n",
    "        pf=ParquetFile(filename)\n",
    "        chunks=pf.iter_batches(batch_size=row_batch_size,columns=feature_names+[target])\n",
    "        # chunksf=pd.read_csv(filename,chunksize=row_batch_size)\n",
    "        chunksL=[]\n",
    "        for chunk in chunks:\n",
    "            chunkf=pa.Table.from_batches([chunk]).to_pandas()\n",
    "            chunksL+=[chunkf]\n",
    "        for df in chunksL[::-1]:\n",
    "            ds_train=MyDS(df[feature_names].values,df[target].values,task='regression')\n",
    "            ds_train.labels=torch.LongTensor([txfTarget(l) for l in ds_train.labels])\n",
    "            dsloader = torch.utils.data.DataLoader(dataset=ds_train,batch_size=batch_size,shuffle=True)        \n",
    "            net,losses,accs,val_accs=TrainInner(net,dsloader,epochs=1,verbose=False,ds_test=ds_test,\n",
    "                               Loss=Loss,device=device,checkpoint_path=None,\n",
    "                               optimizer=optimizer)\n",
    "            if monitor is not None: \n",
    "                monitor.append(e,losses,accs,val_accs)\n",
    "            row_batch_id+=1\n",
    "        if ds_test is not None: val_acc=accuracy(net,ds_test.samples,ds_test.labels,verbose=False)\n",
    "        if verbose: \n",
    "            print(f'Validation accuracy {val_acc} after epoch {e}')\n",
    "            if checkpoint_path is not None: print(f'Saving model checkpoint after epoch {e} at {checkpoint_path}')\n",
    "        if checkpoint_path is not None: torch.save(net.state_dict(),checkpoint_path)\n",
    "    if ds_test is not None: val_acc=accuracy(net,ds_test.samples,ds_test.labels,verbose=False)\n",
    "    print(f'Final validation accuracy {val_acc} after epoch {e}')\n",
    "    return net"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
