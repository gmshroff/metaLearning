{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Models etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import import_ipynb\n",
    "import random\n",
    "import utils\n",
    "utils.hide_toggle('Imports 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from IPython import display\n",
    "utils.hide_toggle('Imports 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute accuracy of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Net,X_test,y_test,verbose=True):\n",
    "    Net.eval()\n",
    "    m = X_test.shape[0]\n",
    "    y_pred = Net(X_test)\n",
    "    predicted = torch.max(y_pred, 1)[1]\n",
    "    correct = (predicted == y_test).float().sum().item()\n",
    "    if verbose: print(correct,m)\n",
    "    accuracy = correct/m\n",
    "    Net.train()\n",
    "    return accuracy\n",
    "utils.hide_toggle('Function: accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_variable(Net,data):\n",
    "    step=0\n",
    "    acc=0\n",
    "    for (X,y) in data:\n",
    "            y_pred = Net(X)\n",
    "            step+=1\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "    a = acc/step\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generic training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train(Net,data,epochs=20,lr=5e-2,Loss=nn.NLLLoss(),verbose=False):\n",
    "    #optimizer = optim.Adam(Net.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    accs = []\n",
    "    for e in range(epochs):\n",
    "        step=0\n",
    "        tot_loss=0.0\n",
    "        acc=0.0\n",
    "        for (X,y) in data:\n",
    "            y_pred = Net(X)\n",
    "            loss = Loss(y_pred,y)\n",
    "            Net.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            Net.optimizer.step()\n",
    "            step+=1\n",
    "            tot_loss+=loss\n",
    "            acc+=accuracy(Net,X,y,verbose=False)\n",
    "        l = tot_loss.item()/step\n",
    "        a = acc/step\n",
    "        losses += [l]\n",
    "        accs += [a]\n",
    "        if verbose: \n",
    "            print('Epoch  % 2d Loss: %2.5e Accuracy: %2.5f'%(e,l,a))\n",
    "            display.clear_output(wait=True)\n",
    "    return Net,losses,accs\n",
    "utils.hide_toggle('Function Train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-layer perceptron with ReLU non-lineartities; for classification or regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,dims=[5,3,2],task='classification',lr=1e-3):\n",
    "        super(MLP,self).__init__()\n",
    "        self.dims=dims\n",
    "        self.n = len(self.dims)-1\n",
    "        self.task=task\n",
    "        self.layers=nn.ModuleList()\n",
    "        for i in range(self.n-1):\n",
    "            self.layers.append(nn.Linear(dims[i],dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        if task=='classification': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.LogSoftmax(dim=1))\n",
    "        elif task=='regression': \n",
    "            self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "            self.layers.append(nn.Linear(dims[i+2],1))\n",
    "        else: self.layers.append(nn.Linear(dims[i+1],dims[i+2]))\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self,x):\n",
    "        for l in self.layers:\n",
    "            x = l(x)\n",
    "        return(x)\n",
    "utils.hide_toggle('Class MLP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent network using RNN/LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.input_size=input_size\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        x = self.logsoft(x)\n",
    "        x=x[:,-1,:]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, lr):\n",
    "        super().__init__()\n",
    "        self.input_size=input_size\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "        x = self.logsoft(x)\n",
    "        x = x[:,-1,:]\n",
    "        return x\n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,lr):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.d_model = 128\n",
    "        self.linear_1 = nn.Linear(self.input_size, self.d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=self.d_model, nhead=8, dim_feedforward=hidden_size)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear_2 = nn.Linear(self.d_model, output_size)\n",
    "        self.logsoft = nn.LogSoftmax(dim=-1)\n",
    "        self.optimizer = optim.Adam(self.parameters(),lr=lr)\n",
    "    def forward(self, x):\n",
    "        if self.input_size==1: x=x.unsqueeze(-1)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.linear_1(x)\n",
    "        x = self.relu(x)\n",
    "        #Positional Encoding\n",
    "        max_len = 25\n",
    "        position = torch.arange(25).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2) * (-np.math.log(10000.0) / self.d_model))\n",
    "        pe = torch.zeros(max_len, 1, self.d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        x = x + pe[:x.size(0)]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(1, 0)\n",
    "        x = x.max(1)[0]\n",
    "        x = self.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.logsoft(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
